{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dust3r.inference import inference, load_model\n",
    "from dust3r.utils.image import load_images\n",
    "from dust3r.image_pairs import make_pairs\n",
    "from dust3r.cloud_opt import global_aligner, GlobalAlignerMode\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "model_path = \"checkpoints/DUSt3R_ViTLarge_BaseDecoder_512_dpt.pth\"\n",
    "device = 'cuda:6'\n",
    "batch_size = 1\n",
    "schedule = 'cosine'\n",
    "lr = 0.01\n",
    "niter = 300\n",
    "\n",
    "model = load_model(model_path, device)\n",
    "# load_images can take a list of images or a directory\n",
    "images = load_images(['croco/assets/Chateau1.png', 'croco/assets/Chateau2.png'], size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = []\n",
    "for _ in range(100):\n",
    "    times = []\n",
    "    t0 = time.time()\n",
    "    pairs = make_pairs(images, scene_graph='complete', prefilter=None, symmetrize=True)\n",
    "    t1 = time.time()\n",
    "    times.append(t1 - t0)\n",
    "    output, ts = inference(pairs, model, device, batch_size=batch_size, return_times=True)\n",
    "    times += ts\n",
    "    # at this stage, you have the raw dust3r predictions\n",
    "    view1, pred1 = output['view1'], output['pred1']\n",
    "    view2, pred2 = output['view2'], output['pred2']\n",
    "    # here, view1, pred1, view2, pred2 are dicts of lists of len(2)\n",
    "    #  -> because we symmetrize we have (im1, im2) and (im2, im1) pairs\n",
    "    # in each view you have:\n",
    "    # an integer image identifier: view1['idx'] and view2['idx']\n",
    "    # the img: view1['img'] and view2['img']\n",
    "    # the image shape: view1['true_shape'] and view2['true_shape']\n",
    "    # an instance string output by the dataloader: view1['instance'] and view2['instance']\n",
    "    # pred1 and pred2 contains the confidence values: pred1['conf'] and pred2['conf']\n",
    "    # pred1 contains 3D points for view1['img'] in view1['img'] space: pred1['pts3d']\n",
    "    # pred2 contains 3D points for view2['img'] in view1['img'] space: pred2['pts3d_in_other_view']\n",
    "\n",
    "    # next we'll use the global_aligner to align the predictions\n",
    "    # depending on your task, you may be fine with the raw output and not need it\n",
    "    # with only two input images, you could use GlobalAlignerMode.PairViewer: it would just convert the output\n",
    "    # if using GlobalAlignerMode.PairViewer, no need to run compute_global_alignment\n",
    "    t0 = time.time()\n",
    "    scene = global_aligner(output, device=device, mode=GlobalAlignerMode.PointCloudOptimizer)\n",
    "    t1 = time.time()\n",
    "    times.append(t1 - t0)\n",
    "    loss = scene.compute_global_alignment(init=\"mst\", niter=niter, schedule=schedule, lr=lr)\n",
    "\n",
    "    # retrieve useful values from scene:\n",
    "    imgs = scene.imgs\n",
    "    t0 = time.time()\n",
    "    focals = scene.get_focals()\n",
    "    poses = scene.get_im_poses()\n",
    "    pts3d = scene.get_pts3d()\n",
    "    confidence_masks = scene.get_masks()\n",
    "    t1 = time.time()\n",
    "    times.append(t1 - t0)\n",
    "    batch.append(times)\n",
    "\n",
    "batch = np.array(batch)\n",
    "print('Pairing, Encoder, Decoder, Downstream Head')\n",
    "print(f'batch times: {batch.mean(0)}')\n",
    "print(f'batch times std: {batch.std(0)}')\n",
    "print(f'% of time spent in each step: {batch.mean(0) / batch.mean(0).sum() * 100}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Pairing         Encoder        Decoder        Head           Scene          Scene Parsing\n",
    "    17.82us         19.93ms        26.93ms        11.24ms        72.59ms        6.69ms\n",
    "    0.01%           14.51%         19.59%         8.18%          52.83%         4.87%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Pairing         Encoder        Decoder        Head           Scene          Scene Parsing\n",
    "    19.42us         16.71ms        23.73ms        14.48ms        20.95ms        6.98ms\n",
    "    6.88us          2.81ms         2.23ms         2.51ms         9.37ms         1.19ms\n",
    "    0.02%           20.17%         28.64%         17.48%         25.28%         8.42%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.01 + 14.51 + 19.59 + 8.18 + 52.83 + 4.87"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = [16.71, 23.73, 14.48]\n",
    "s = [19.93, 26.93, 11.24]\n",
    "\n",
    "[i / sum(s) * 100 for i in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-save Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open('/ssd1/sa58728/dust3r/data/co3d_subset_processed/apple/110_13051_23361/images/frame000001.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('/ssd1/sa58728/dust3r/data/co3d_subset_processed/selected_seqs_train.json') as f:\n",
    "    train = json.load(f)\n",
    "with open('/ssd1/sa58728/dust3r/data/co3d_subset_processed/selected_seqs_test.json') as f:\n",
    "    test = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(i, train[i].keys()) for i in train.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jsondiff import diff\n",
    "diff(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "list_1 = glob.glob(r\"/ssd1/sa58728/dust3r/data/co3d_subset_processed/*/*/images/*.jpg\")\n",
    "list_2 = glob.glob(r\"/ssd1/sa58728/dust3r/data/co3d_subset_processed/*/*/images/*.npy\")\n",
    "list_3 = glob.glob(r\"/ssd1/sa58728/dust3r/data/co3d_subset_processed/*/*/images/*.npz\")\n",
    "len(list_1), len(list_2), len(list_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_1 = [f[:-4] for f in list_1]\n",
    "list_2 = [f[:-4] for f in list_2]\n",
    "list_3 = [f[:-4] for f in list_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = set(list_2)\n",
    "temp = [x for x in list_1 if x not in s]\n",
    "print(sorted(temp))\n",
    "len(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_1 == list_2, list_1 == list_3, list_2 == list_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "for i in list_2:\n",
    "    os.remove(i)\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from dust3r.model import AsymmetricCroCo3DStereo, inf  # noqa: F401, needed when loading the model\n",
    "\n",
    "model = AsymmetricCroCo3DStereo(\n",
    "    pos_embed='RoPE100',\n",
    "    img_size=(224, 224),\n",
    "    head_type='linear',\n",
    "    output_mode='pts3d', \n",
    "    depth_mode=('exp', -inf, inf), \n",
    "    conf_mode=('exp', 1, inf), \n",
    "    enc_embed_dim=192, \n",
    "    enc_depth=12, \n",
    "    enc_num_heads=3, \n",
    "    dec_embed_dim=768, \n",
    "    dec_depth=12, \n",
    "    dec_num_heads=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_modules = [model.patch_embed, model.mask_generator, model.rope, model.enc_blocks, model.enc_norm]\n",
    "train_params = torch.nn.ParameterList([p for m in train_modules for p in m.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(train_params, lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from dust3r.model import AsymmetricCroCo3DStereo, inf  # noqa: F401, needed when loading the model\n",
    "from dust3r.inference import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_KD = \"AsymmetricCroCo3DStereo(pos_embed='RoPE100', img_size=(224, 224), head_type='dpt', \\\n",
    "            output_mode='pts3d', depth_mode=('exp', -inf, inf), conf_mode=('exp', 1, inf), \\\n",
    "            enc_embed_dim=384, enc_depth=12, enc_num_heads=6, dec_embed_dim=768, dec_depth=12, dec_num_heads=12, adapter=True)\"\n",
    "MODEL_NEW = \"AsymmetricCroCo3DStereo(pos_embed='RoPE100', img_size=(224, 224), head_type='dpt', \\\n",
    "            output_mode='pts3d', depth_mode=('exp', -inf, inf), conf_mode=('exp', 1, inf), \\\n",
    "            enc_embed_dim=384, enc_depth=12, enc_num_heads=6, dec_embed_dim=768, dec_depth=12, dec_num_heads=12, adapter=True)\"\n",
    "\n",
    "CKPT = \"checkpoints/DUSt3R_ViTLarge_BaseDecoder_512_dpt.pth\"\n",
    "CKPT_KD = \"log/train_10/checkpoint-best.pth\"\n",
    "CKPT_NEW = \"checkpoints/DUSt3R_ViTSmall_BaseDecoder_512_dpt_kd.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(CKPT, 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kd = eval(MODEL_KD)\n",
    "ckpt_kd = torch.load(CKPT_KD)['model']\n",
    "print(model_kd.load_state_dict(ckpt_kd, strict=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_new = eval(MODEL_NEW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_list = ['decoder_embed', 'dec_norm', 'dec_blocks', 'dec_norm', 'dec_blocks2', 'downstream_head1', 'downstream_head2']\n",
    "module_list_kd = ['patch_embed', 'mask_generator', 'rope', 'enc_blocks', 'enc_norm', 'adapter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in module_list:\n",
    "    getattr(model_new, m).load_state_dict(getattr(model, m).state_dict(), strict=True)\n",
    "model_new.mask_token = model.mask_token\n",
    "\n",
    "for m in module_list_kd:\n",
    "    getattr(model_new, m).load_state_dict(getattr(model_kd, m).state_dict(), strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(model_1, model_2):\n",
    "    models_differ = 0\n",
    "    for key_item_1, key_item_2 in zip(model_1.state_dict().items(), model_2.state_dict().items()):\n",
    "        if torch.equal(key_item_1[1], key_item_2[1]):\n",
    "            pass\n",
    "        else:\n",
    "            models_differ += 1\n",
    "            if (key_item_1[0] == key_item_2[0]):\n",
    "                print('Mismatch found at', key_item_1[0], key_item_2[0])\n",
    "            else:\n",
    "                print('Error at', key_item_1[0], key_item_2[0])\n",
    "            return False\n",
    "    if models_differ == 0:\n",
    "        # print('Models match perfectly! :)')\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m, k in zip(model_kd.named_children(), model_new.named_children()):\n",
    "    print(m[0], compare_models(m[1],k[1]), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [x for x in model_new.named_children() if x[0] != 'adapter']\n",
    "for m, k in zip(model.named_children(), l):\n",
    "    print(m[0], compare_models(m[1],k[1]), '\\n')\n",
    "    # except:\n",
    "    #     print(m[0], \"Size Mismatch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_new.state_dict(), CKPT_NEW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_new = torch.load(CKPT_NEW)\n",
    "print(model_kd.load_state_dict(ckpt_new, strict=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_enc_dec(model_str, device):\n",
    "    teacher = load_model(\"checkpoints/DUSt3R_ViTLarge_BaseDecoder_512_dpt.pth\", device)\n",
    "    teacher.eval()\n",
    "\n",
    "    model = eval(model_str)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    module_list = ['decoder_embed', 'dec_norm', 'dec_blocks', 'dec_norm', 'dec_blocks2', 'downstream_head1', 'downstream_head2']\n",
    "    for m in module_list:\n",
    "        getattr(model, m).load_state_dict(getattr(teacher, m).state_dict(), strict=True)\n",
    "\n",
    "    return teacher, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher, model = build_model_enc_dec(MODEL_NEW, 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [x for x in model.named_children() if x[0] != 'adapter']\n",
    "for m, k in zip(teacher.named_children(), l):\n",
    "    print(m[0], compare_models(m[1],k[1]), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open('/ssd1/sa58728/dust3r/data/co3d_subset_processed/apple/110_13051_23361/images/frame000060.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open('/ssd1/sa58728/dust3r/data/co3d_subset_processed/apple/110_13051_23361/images/frame000030.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = torch.load('log/train_w_0/checkpoint-1.pth', map_location='cpu')['model']\n",
    "w1 = torch.load('log/train_x_1/checkpoint-2.pth', map_location='cpu')['model']\n",
    "w2 = torch.load('checkpoints/DUSt3R_ViTLarge_BaseDecoder_512_dpt.pth', map_location='cpu')['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h0 = torch.load('log_kd/log_h/ckpt/iter_2475.pth', map_location='cpu')\n",
    "h1 = torch.load('log_kd/log_h/ckpt/iter_4950.pth', map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, j in zip(w2.items(), w0.items()):\n",
    "    if torch.equal(i[1], j[1]):\n",
    "        print(i[0], torch.equal(i[1], j[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0['model']['mask_token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "w = torch.load('log/train_10/checkpoint-last.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w['best_so_far']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Habitat MP3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# i = cv2.imread('/ssd1/wenyan/scannetpp_processed/d6d9ddb03f/depths/DSC05114.png')\n",
    "i = cv2.imread('/mnt/wenyan/scannetpp_processed/d6d9ddb03f/depths/DSC05114.png')\n",
    "i = i * 255.0 / i.max()\n",
    "plt.imshow(i / 255.0, cmap='inferno')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i.min(), i.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ScanNet++ Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = '/ssd1/wenyan/scannetpp_processed'\n",
    "\n",
    "def split_train_test(scenes, split=0.8):\n",
    "    n = len(scenes)\n",
    "    train = scenes[:int(n * split)]\n",
    "    test = scenes[int(n * split):]\n",
    "    return train, test\n",
    "\n",
    "def get_scene_frames(scene, base_path):\n",
    "    frames = os.listdir(os.path.join(base_path, scene, 'images'))\n",
    "    return [int(f.split('.')[0].split(\"DSC\")[1].lstrip(\"0\")) for f in frames if f.endswith('.JPG')]\n",
    "\n",
    "\n",
    "def get_split_scenes(split):\n",
    "    scenes = {}\n",
    "    for scene in split:\n",
    "        if os.path.isdir(os.path.join(BASE_PATH, scene)):\n",
    "            frames = get_scene_frames(scene, BASE_PATH)\n",
    "            scenes[scene] = frames\n",
    "    return scenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenes = os.listdir(BASE_PATH)\n",
    "train, test = split_train_test(scenes)\n",
    "train = get_split_scenes(train)\n",
    "test = get_split_scenes(test)\n",
    "\n",
    "with open('selected_seqs_train.json', 'w') as file:\n",
    "    json.dump(train, file)\n",
    "with open('selected_seqs_test.json', 'w') as file:\n",
    "    json.dump(test, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dust3r.datasets import ScanNet\n",
    "\n",
    "dataset = ScanNet(split='train', ROOT='/ssd1/wenyan/scannetpp_processed', aug_crop=16, mask_bg='rand', resolution=224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dataset:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL3DV Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = '/ssd1/sa58728/dust3r/data/DL3DV-10K/'\n",
    "\n",
    "def split_train_test(scenes, split=0.8):\n",
    "    n = len(scenes)\n",
    "    train = scenes[:int(n * split)]\n",
    "    test = scenes[int(n * split):]\n",
    "    return train, test\n",
    "\n",
    "def get_scene_frames(scene, base_path):\n",
    "    frames = os.listdir(os.path.join(base_path, scene, 'gaussian_splat/images_4'))\n",
    "    return [int(f.split('.')[0].split(\"frame_\")[1].lstrip(\"0\")) for f in frames if f.endswith('.png')]\n",
    "\n",
    "\n",
    "def get_split_scenes(split):\n",
    "    scenes = {}\n",
    "    for scene in split:\n",
    "        if os.path.isdir(os.path.join(BASE_PATH, scene)):\n",
    "            frames = get_scene_frames(scene, BASE_PATH)\n",
    "            scenes[scene] = frames\n",
    "    return scenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenes = os.listdir(BASE_PATH)\n",
    "train, test = split_train_test(scenes)\n",
    "train = get_split_scenes(train)\n",
    "test = get_split_scenes(test)\n",
    "\n",
    "with open('selected_seqs_train.json', 'w') as file:\n",
    "    json.dump(train, file)\n",
    "with open('selected_seqs_test.json', 'w') as file:\n",
    "    json.dump(test, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dust3r.datasets import DL3DV\n",
    "\n",
    "dataset = DL3DV(split='train', ROOT='/ssd1/sa58728/dust3r/data/DL3DV-10K', aug_crop=16, mask_bg='rand', resolution=224)\n",
    "\n",
    "for i in dataset:\n",
    "    print(i[0]['img'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(i[0]['img'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MegaDepth Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = '/ssd1/sa58728/dust3r/data/MegaDepth_v1/'\n",
    "\n",
    "def split_train_test(scenes, split=0.8):\n",
    "    n = len(scenes)\n",
    "    train = scenes[:int(n * split)]\n",
    "    test = scenes[int(n * split):]\n",
    "    return train, test\n",
    "\n",
    "def get_scene_frames(scene, base_path):\n",
    "    if scene.endswith('list'):\n",
    "        return []\n",
    "    try:\n",
    "        frames = os.listdir(os.path.join(base_path, scene, 'dense0/imgs/'))\n",
    "    except:\n",
    "        frames = os.listdir(os.path.join(base_path, scene, 'dense1/imgs/'))\n",
    "    return [f.split('.')[0] for f in frames if f.endswith('.jpg')]\n",
    "\n",
    "\n",
    "def get_split_scenes(split):\n",
    "    scenes = {}\n",
    "    for scene in split:\n",
    "        if os.path.isdir(os.path.join(BASE_PATH, scene)):\n",
    "            frames = get_scene_frames(scene, BASE_PATH)\n",
    "            scenes[scene] = frames\n",
    "    return scenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenes = os.listdir(BASE_PATH)\n",
    "train, test = split_train_test(scenes)\n",
    "train = get_split_scenes(train)\n",
    "test = get_split_scenes(test)\n",
    "\n",
    "with open('selected_seqs_train.json', 'w') as file:\n",
    "    json.dump(train, file)\n",
    "with open('selected_seqs_test.json', 'w') as file:\n",
    "    json.dump(test, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dust3r.datasets import MegaDepth\n",
    "\n",
    "dataset = MegaDepth(split='train', ROOT='/ssd1/sa58728/dust3r/data/MegaDepth_v1', aug_crop=16, mask_bg='rand', resolution=224)\n",
    "\n",
    "for i in dataset:\n",
    "    print(i[0]['img'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(i[0]['img'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEAN = 3\n",
    "STD = 3\n",
    "\n",
    "m, n = [], []\n",
    "for i in range(100000):\n",
    "    m.append(int(rng.normal(loc=0.0, scale=STD)) + MEAN)\n",
    "    n.append(int(rng.normal(loc=0.0, scale=STD)))\n",
    "o = [m - n for m, n in zip(m, n)] + [n - m for m, n in zip(m, n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v2\n",
    "bins = plt.hist(o, bins=range(-24,30), align='left', density=True, cumulative=False)\n",
    "# plt.xticks(np.arange(-21, 22, 3))\n",
    "# plt.yticks(np.arange(0, 1.1, 0.05))\n",
    "# plt.grid(True, alpha=0.3)\n",
    "# plt.hlines(0.025, -20, 22, color='r', alpha=0.5)\n",
    "# plt.hlines(0.975, -20, 22, color='r', alpha=0.5)\n",
    "# plt.hlines(0.25, -20, 22, color='r', alpha=0.5)\n",
    "# plt.hlines(0.75, -20, 22, color='r', alpha=0.5)\n",
    "# plt.vlines(-9, 0, 1, color='r', alpha=0.5)\n",
    "# plt.vlines(-3, 0, 1, color='r', alpha=0.5)\n",
    "# plt.vlines(3, 0, 1, color='r', alpha=0.5)\n",
    "# plt.vlines(9, 0, 1, color='r', alpha=0.5)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v1\n",
    "bins = plt.hist(o, bins=range(-24,30), align='left', density=True, cumulative=True)\n",
    "plt.xticks(np.arange(-20, 22, 5))\n",
    "plt.yticks(np.arange(0, 1.1, 0.05))\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.hlines(0.025, -20, 22, color='r', alpha=0.5)\n",
    "plt.hlines(0.975, -20, 22, color='r', alpha=0.5)\n",
    "plt.hlines(0.25, -20, 22, color='r', alpha=0.5)\n",
    "plt.hlines(0.75, -20, 22, color='r', alpha=0.5)\n",
    "plt.vlines(-15, 0, 1, color='r', alpha=0.5)\n",
    "plt.vlines(-5, 0, 1, color='r', alpha=0.5)\n",
    "plt.vlines(15, 0, 1, color='r', alpha=0.5)\n",
    "plt.vlines(5, 0, 1, color='r', alpha=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "combinations = [\n",
    "    (i, j)\n",
    "    for i, j in itertools.combinations(range(1000), 2)\n",
    "    if 0 < abs(i-j) <= 30 and abs(i-j) % 5 == 0\n",
    "]\n",
    "o = []\n",
    "for i, j in combinations:\n",
    "    imgs_idxs = [max(0, min(im_idx + rng.integers(-4, 5), 1000)) for im_idx in [i, j]]\n",
    "    o.append(imgs_idxs[0] - imgs_idxs[1])\n",
    "    o.append(imgs_idxs[1] - imgs_idxs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = plt.hist(o, bins=range(-40,40), align='left', density=True, cumulative=True)\n",
    "plt.xticks(np.arange(-40, 40, 5))\n",
    "plt.yticks(np.arange(0, 1.1, 0.05))\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.hlines(0.025, -40, 40, color='r', alpha=0.5)\n",
    "plt.hlines(0.975, -40, 40, color='r', alpha=0.5)\n",
    "plt.hlines(0.25, -40, 40, color='r', alpha=0.5)\n",
    "plt.hlines(0.75, -40, 40, color='r', alpha=0.5)\n",
    "plt.vlines(-32, 0, 1, color='r', alpha=0.5)\n",
    "plt.vlines(-18, 0, 1, color='r', alpha=0.5)\n",
    "plt.vlines(18, 0, 1, color='r', alpha=0.5)\n",
    "plt.vlines(32, 0, 1, color='r', alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Pair Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dust3r.datasets import get_data_loader  # noqa\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "OUT_FOLDER = \"pairs\"\n",
    "DATASET = \"DL3DV\"\n",
    "GAUSSIAN = True\n",
    "\n",
    "if DATASET == \"Co3D\":\n",
    "    TRAIN_DATA = f\"1000 @ Co3d(split='train', ROOT='/ssd1/wenyan/co3d_2_cat_processed', aug_crop=16, mask_bg='rand', resolution=224, transform=ImgNorm, gaussian_frames={GAUSSIAN})\"\n",
    "elif DATASET == \"ScanNet\":\n",
    "    TRAIN_DATA = f\"1000 @ ScanNet(split='train', ROOT='/ssd1/wenyan/scannetpp_processed', aug_crop=16, mask_bg='rand', resolution=224, transform=ImgNorm, gaussian_frames={GAUSSIAN})\"\n",
    "elif DATASET == \"DL3DV\":\n",
    "    TRAIN_DATA = f\"1000 @ DL3DV(split='train', ROOT='/ssd1/sa58728/dust3r/data/DL3DV-10K', aug_crop=16, mask_bg='rand', resolution=224, transform=ImgNorm, gaussian_frames={GAUSSIAN})\"\n",
    "\n",
    "loader = get_data_loader(\n",
    "    TRAIN_DATA,\n",
    "    batch_size=1,\n",
    "    num_workers=4,\n",
    "    pin_mem=True,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "if hasattr(loader, 'dataset') and hasattr(loader.dataset, 'set_epoch'):\n",
    "    loader.dataset.set_epoch(0)\n",
    "if hasattr(loader, 'sampler') and hasattr(loader.sampler, 'set_epoch'):\n",
    "    loader.sampler.set_epoch(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image_pair(pair, plot=False):\n",
    "    name1 = pair[0]['instance'][0].split('.')[0]\n",
    "    name2 = pair[1]['instance'][0].split('.')[0]\n",
    "    name = f\"{name1}_{name2}\"\n",
    "    os.makedirs(f\"{OUT_FOLDER}/{DATASET}/{name}\", exist_ok=True)\n",
    "\n",
    "    img1 = ((pair[0]['img'][0] * 0.5) + 0.5)\n",
    "    img2 = ((pair[1]['img'][0] * 0.5) + 0.5)\n",
    "\n",
    "    save_image(img1, f\"{OUT_FOLDER}/{DATASET}/{name}/{name1}.png\")\n",
    "    save_image(img2, f\"{OUT_FOLDER}/{DATASET}/{name}/{name2}.png\")\n",
    "\n",
    "    if plot:\n",
    "        plt.imshow(img1.permute(1, 2, 0))\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "        plt.imshow(img2.permute(1, 2, 0))\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, pair in enumerate(loader, 1):\n",
    "    save_image_pair(pair)\n",
    "    if i == 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RoMa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "xFormers not available\n",
      "xFormers not available\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "# RoMa\n",
    "import torch.nn.functional as F\n",
    "from RoMa.roma.utils.utils import tensor_to_pil\n",
    "from RoMa.roma import roma_outdoor\n",
    "# DUSt3R\n",
    "torch.backends.cuda.matmul.allow_tf32 = True  # for gpu >= Ampere and pytorch >= 1.12\n",
    "from dust3r.losses import *  # noqa: F401, needed when loading the model\n",
    "from dust3r.inference import loss_of_one_batch, load_model\n",
    "from dust3r.utils.image import load_images\n",
    "from dust3r.image_pairs import make_pairs\n",
    "from dust3r.utils.device import to_cpu, collate_with_cat\n",
    "from test_pairs import *\n",
    "\n",
    "CKPT = \"checkpoints/DUSt3R_ViTLarge_BaseDecoder_512_dpt.pth\"\n",
    "MODEL_KD = \"AsymmetricCroCo3DStereo(pos_embed='RoPE100', img_size=(224, 224), head_type='dpt', \\\n",
    "            output_mode='pts3d', depth_mode=('exp', -inf, inf), conf_mode=('exp', 1, inf), \\\n",
    "            enc_embed_dim=384, enc_depth=12, enc_num_heads=6, dec_embed_dim=768, dec_depth=12, dec_num_heads=12, adapter=True)\"\n",
    "CKPT_KD = \"log/train_2/checkpoint-best.pth\"\n",
    "TEST_CRITERION = \"ConfLoss(Regr3D(L21, norm_mode='avg_dis', kd=True), alpha=0.2) + Regr3D_ScaleShiftInv(L21, gt_scale=True, kd=True)\"\n",
    "\n",
    "device = torch.device('cpu')\n",
    "device = torch.device('cuda:6' if torch.cuda.is_available() else 'cpu')\n",
    "img_path = \"test/RoMa/\"\n",
    "im1_path = img_path + \"DSC00410.png\"\n",
    "im2_path = img_path + \"DSC09985.png\"\n",
    "save_path = \"./roma.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sa58728/anaconda3/envs/dust3r/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/sa58728/anaconda3/envs/dust3r/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using coarse resolution (224, 224), and upsample res (224, 224)\n",
      "... loading model from checkpoints/DUSt3R_ViTLarge_BaseDecoder_512_dpt.pth\n",
      "instantiating : AsymmetricCroCo3DStereo(enc_depth=24, dec_depth=12, enc_embed_dim=1024, dec_embed_dim=768, enc_num_heads=16, dec_num_heads=12, pos_embed='RoPE100', patch_embed_cls='PatchEmbedDust3R', img_size=(512, 512), head_type='dpt', output_mode='pts3d', depth_mode=('exp', -inf, inf), conf_mode=('exp', 1, inf), landscape_only=False)\n",
      "<All keys matched successfully>\n",
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "roma_model = roma_outdoor(device=device, coarse_res=224, upsample_res=224)\n",
    "H, W = roma_model.get_output_resolution()\n",
    "\n",
    "im1 = Image.open(im1_path).resize((W, H))\n",
    "im2 = Image.open(im2_path).resize((W, H))\n",
    "\n",
    "teacher, model = load_pretrained(MODEL_KD, CKPT, CKPT_KD, device)\n",
    "test_criterion = eval(TEST_CRITERION).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Loading images from test/RoMa/\n",
      " - adding DSC00410.png with resolution 224x224 --> 224x224\n",
      " - adding DSC09985.png with resolution 224x224 --> 224x224\n",
      " (Found 2 images)\n",
      "Details: {'conf_loss': -0.39856094121932983, 'Regr3D_pts3d': 0.0, 'Regr3D_ScaleShiftInv_pts3d': 9.307386861223677e-09, 'kd': 0.0, 'loss': -0.7971218824386597}\n"
     ]
    }
   ],
   "source": [
    "images = load_images(img_path, size=224)\n",
    "pairs = make_pairs(images, scene_graph='complete', prefilter=None, symmetrize=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    result = loss_of_one_batch(collate_with_cat(pairs), teacher, test_criterion, device,\n",
    "                            symmetrize_batch=False, features=True,\n",
    "                            kd=True, kd_out=True, teacher=teacher, lmd=10)\n",
    "    \n",
    "result = to_cpu(result)\n",
    "loss_value, loss_details = result['loss']  # criterion returns two values\n",
    "loss_details['loss'] = loss_value.item()\n",
    "print(f\"Details: {loss_details}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 224, 224, 3]), torch.Size([1, 224, 224, 3]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['pred1']['pts3d'].shape, result['pred2']['pts3d_in_other_view'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match\n",
    "warp, certainty = roma_model.match(im1_path, im2_path, device=device)\n",
    "warp, certainty = warp.reshape(-1, 2*H*W, 4), certainty.reshape(-1, 2*H*W)\n",
    "warp_filter = warp[certainty > 0.55]\n",
    "# out, valid = roma_model.sample(warp, certainty)\n",
    "kptsA, kptsB = roma_model.to_pixel_coordinates(warp_filter, H, W, H, W)\n",
    "kptsA, kptsB = kptsA.cpu().numpy().astype('int'), kptsB.cpu().numpy().astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warp_filter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(certainty.reshape(H,2*W).cpu().numpy()>0.5, cmap='inferno')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(certainty.cpu().numpy().flatten(), cumulative=True, bins=100)\n",
    "plt.hlines(90000, 0, 1, color='r', alpha=0.5)\n",
    "plt.vlines(0.55, 0, 100000, color='r', alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warp.min(), warp.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kptsA, kptsB = roma_model.to_pixel_coordinates(warp, H, W, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kptsA.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling not needed, but can be done with model.sample(warp, certainty)\n",
    "x1 = (torch.tensor(np.array(im1)) / 255).to(device).permute(2, 0, 1)\n",
    "x2 = (torch.tensor(np.array(im2)) / 255).to(device).permute(2, 0, 1)\n",
    "\n",
    "# take only the matching part from images and copy colors (xyz values once normalized)\n",
    "im1_transfer_rgb = F.grid_sample(\n",
    "    x1[None], warp[:, W:, :2][None], mode=\"bilinear\", align_corners=False\n",
    ")[0]\n",
    "im2_transfer_rgb = F.grid_sample(\n",
    "    x2[None], warp[:,:W, 2:][None], mode=\"bilinear\", align_corners=False\n",
    ")[0]\n",
    "warp_im = torch.cat((im2_transfer_rgb,im1_transfer_rgb),dim=2)\n",
    "white_im = torch.ones((H,2*W),device=device)\n",
    "vis_im = certainty * warp_im + (1 - certainty) * white_im\n",
    "tensor_to_pil(vis_im, unnormalize=False).save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kptsB.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 2500\n",
    "print(kptsA[i])\n",
    "plt.imshow(im1)\n",
    "plt.scatter(*kptsA[i], c='r', s=10)\n",
    "plt.show()\n",
    "plt.imshow(im2)\n",
    "plt.scatter(*kptsB[i], c='r', s=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1240\n",
    "print(kptsA[i])\n",
    "plt.imshow((pairs[0][1]['img'][0].permute(1, 2, 0)*0.5)+0.5)\n",
    "plt.scatter(*kptsA[i], c='r', s=10)\n",
    "plt.show()\n",
    "plt.imshow((pairs[0][0]['img'][0].permute(1, 2, 0)*0.5)+0.5)\n",
    "plt.scatter(*kptsB[i], c='r', s=10)\n",
    "plt.show()\n",
    "plt.imshow(result['pred2']['pts3d_in_other_view'][0,:,:,-1])\n",
    "plt.scatter(*kptsA[i], c='r', s=10)\n",
    "plt.show()\n",
    "plt.imshow(result['pred1']['pts3d'][0,:,:,-1])\n",
    "plt.scatter(*kptsB[i], c='r', s=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['pred1']['pts3d'][0,kptsB[:,0],kptsB[:,1]].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = result['pred1']['pts3d'][0,kptsB[:,0],kptsB[:,1]] # * valid[:,None].cpu()\n",
    "p2 = result['pred2']['pts3d_in_other_view'][0,kptsA[:,0],kptsA[:,1]] # * valid[:,None].cpu()\n",
    "print(p1.shape, p2.shape, (p1 - p2).abs().mean(), ((p1 - p2)**2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kptsA[i].astype('int'), kptsA[i], kptsB[i].astype('int'), kptsB[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['pred1']['pts3d'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch RoMa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dust3r",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
