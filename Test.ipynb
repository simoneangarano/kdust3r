{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dust3r.inference import inference, load_model\n",
    "from dust3r.utils.image import load_images\n",
    "from dust3r.image_pairs import make_pairs\n",
    "from dust3r.cloud_opt import global_aligner, GlobalAlignerMode\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "model_path = \"checkpoints/DUSt3R_ViTLarge_BaseDecoder_512_dpt.pth\"\n",
    "device = 'cuda:6'\n",
    "batch_size = 1\n",
    "schedule = 'cosine'\n",
    "lr = 0.01\n",
    "niter = 300\n",
    "\n",
    "model = load_model(model_path, device)\n",
    "# load_images can take a list of images or a directory\n",
    "images = load_images(['croco/assets/Chateau1.png', 'croco/assets/Chateau2.png'], size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = []\n",
    "for _ in range(100):\n",
    "    times = []\n",
    "    t0 = time.time()\n",
    "    pairs = make_pairs(images, scene_graph='complete', prefilter=None, symmetrize=True)\n",
    "    t1 = time.time()\n",
    "    times.append(t1 - t0)\n",
    "    output, ts = inference(pairs, model, device, batch_size=batch_size, return_times=True)\n",
    "    times += ts\n",
    "    # at this stage, you have the raw dust3r predictions\n",
    "    view1, pred1 = output['view1'], output['pred1']\n",
    "    view2, pred2 = output['view2'], output['pred2']\n",
    "    # here, view1, pred1, view2, pred2 are dicts of lists of len(2)\n",
    "    #  -> because we symmetrize we have (im1, im2) and (im2, im1) pairs\n",
    "    # in each view you have:\n",
    "    # an integer image identifier: view1['idx'] and view2['idx']\n",
    "    # the img: view1['img'] and view2['img']\n",
    "    # the image shape: view1['true_shape'] and view2['true_shape']\n",
    "    # an instance string output by the dataloader: view1['instance'] and view2['instance']\n",
    "    # pred1 and pred2 contains the confidence values: pred1['conf'] and pred2['conf']\n",
    "    # pred1 contains 3D points for view1['img'] in view1['img'] space: pred1['pts3d']\n",
    "    # pred2 contains 3D points for view2['img'] in view1['img'] space: pred2['pts3d_in_other_view']\n",
    "\n",
    "    # next we'll use the global_aligner to align the predictions\n",
    "    # depending on your task, you may be fine with the raw output and not need it\n",
    "    # with only two input images, you could use GlobalAlignerMode.PairViewer: it would just convert the output\n",
    "    # if using GlobalAlignerMode.PairViewer, no need to run compute_global_alignment\n",
    "    t0 = time.time()\n",
    "    scene = global_aligner(output, device=device, mode=GlobalAlignerMode.PointCloudOptimizer)\n",
    "    t1 = time.time()\n",
    "    times.append(t1 - t0)\n",
    "    loss = scene.compute_global_alignment(init=\"mst\", niter=niter, schedule=schedule, lr=lr)\n",
    "\n",
    "    # retrieve useful values from scene:\n",
    "    imgs = scene.imgs\n",
    "    t0 = time.time()\n",
    "    focals = scene.get_focals()\n",
    "    poses = scene.get_im_poses()\n",
    "    pts3d = scene.get_pts3d()\n",
    "    confidence_masks = scene.get_masks()\n",
    "    t1 = time.time()\n",
    "    times.append(t1 - t0)\n",
    "    batch.append(times)\n",
    "\n",
    "batch = np.array(batch)\n",
    "print('Pairing, Encoder, Decoder, Downstream Head')\n",
    "print(f'batch times: {batch.mean(0)}')\n",
    "print(f'batch times std: {batch.std(0)}')\n",
    "print(f'% of time spent in each step: {batch.mean(0) / batch.mean(0).sum() * 100}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Pairing         Encoder        Decoder        Head           Scene          Scene Parsing\n",
    "    17.82us         19.93ms        26.93ms        11.24ms        72.59ms        6.69ms\n",
    "    0.01%           14.51%         19.59%         8.18%          52.83%         4.87%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Pairing         Encoder        Decoder        Head           Scene          Scene Parsing\n",
    "    19.42us         16.71ms        23.73ms        14.48ms        20.95ms        6.98ms\n",
    "    6.88us          2.81ms         2.23ms         2.51ms         9.37ms         1.19ms\n",
    "    0.02%           20.17%         28.64%         17.48%         25.28%         8.42%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.01 + 14.51 + 19.59 + 8.18 + 52.83 + 4.87"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = [16.71, 23.73, 14.48]\n",
    "s = [19.93, 26.93, 11.24]\n",
    "\n",
    "[i / sum(s) * 100 for i in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-save Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open('/ssd1/sa58728/dust3r/data/co3d_subset_processed/apple/110_13051_23361/images/frame000001.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('/ssd1/sa58728/dust3r/data/co3d_subset_processed/selected_seqs_train.json') as f:\n",
    "    train = json.load(f)\n",
    "with open('/ssd1/sa58728/dust3r/data/co3d_subset_processed/selected_seqs_test.json') as f:\n",
    "    test = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(i, train[i].keys()) for i in train.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jsondiff import diff\n",
    "diff(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "list_1 = glob.glob(r\"/ssd1/sa58728/dust3r/data/co3d_subset_processed/*/*/images/*.jpg\")\n",
    "list_2 = glob.glob(r\"/ssd1/sa58728/dust3r/data/co3d_subset_processed/*/*/images/*.npy\")\n",
    "list_3 = glob.glob(r\"/ssd1/sa58728/dust3r/data/co3d_subset_processed/*/*/images/*.npz\")\n",
    "len(list_1), len(list_2), len(list_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_1 = [f[:-4] for f in list_1]\n",
    "list_2 = [f[:-4] for f in list_2]\n",
    "list_3 = [f[:-4] for f in list_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = set(list_2)\n",
    "temp = [x for x in list_1 if x not in s]\n",
    "print(sorted(temp))\n",
    "len(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_1 == list_2, list_1 == list_3, list_2 == list_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "for i in list_2:\n",
    "    os.remove(i)\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from dust3r.model import AsymmetricCroCo3DStereo, inf  # noqa: F401, needed when loading the model\n",
    "\n",
    "model = AsymmetricCroCo3DStereo(\n",
    "    pos_embed='RoPE100',\n",
    "    img_size=(224, 224),\n",
    "    head_type='linear',\n",
    "    output_mode='pts3d', \n",
    "    depth_mode=('exp', -inf, inf), \n",
    "    conf_mode=('exp', 1, inf), \n",
    "    enc_embed_dim=192, \n",
    "    enc_depth=12, \n",
    "    enc_num_heads=3, \n",
    "    dec_embed_dim=768, \n",
    "    dec_depth=12, \n",
    "    dec_num_heads=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_modules = [model.patch_embed, model.mask_generator, model.rope, model.enc_blocks, model.enc_norm]\n",
    "train_params = torch.nn.ParameterList([p for m in train_modules for p in m.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(train_params, lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from dust3r.model import AsymmetricCroCo3DStereo, inf  # noqa: F401, needed when loading the model\n",
    "from dust3r.inference import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_KD = \"AsymmetricCroCo3DStereo(pos_embed='RoPE100', img_size=(224, 224), head_type='dpt', \\\n",
    "            output_mode='pts3d', depth_mode=('exp', -inf, inf), conf_mode=('exp', 1, inf), \\\n",
    "            enc_embed_dim=384, enc_depth=12, enc_num_heads=6, dec_embed_dim=768, dec_depth=12, dec_num_heads=12, adapter=True)\"\n",
    "MODEL_NEW = \"AsymmetricCroCo3DStereo(pos_embed='RoPE100', img_size=(224, 224), head_type='dpt', \\\n",
    "            output_mode='pts3d', depth_mode=('exp', -inf, inf), conf_mode=('exp', 1, inf), \\\n",
    "            enc_embed_dim=384, enc_depth=12, enc_num_heads=6, dec_embed_dim=768, dec_depth=12, dec_num_heads=12, adapter=True)\"\n",
    "\n",
    "CKPT = \"checkpoints/DUSt3R_ViTLarge_BaseDecoder_512_dpt.pth\"\n",
    "CKPT_KD = \"log/train_10/checkpoint-best.pth\"\n",
    "CKPT_NEW = \"checkpoints/DUSt3R_ViTSmall_BaseDecoder_512_dpt_kd.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(CKPT, 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kd = eval(MODEL_KD)\n",
    "ckpt_kd = torch.load(CKPT_KD)['model']\n",
    "print(model_kd.load_state_dict(ckpt_kd, strict=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_new = eval(MODEL_NEW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_list = ['decoder_embed', 'dec_norm', 'dec_blocks', 'dec_norm', 'dec_blocks2', 'downstream_head1', 'downstream_head2']\n",
    "module_list_kd = ['patch_embed', 'mask_generator', 'rope', 'enc_blocks', 'enc_norm', 'adapter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in module_list:\n",
    "    getattr(model_new, m).load_state_dict(getattr(model, m).state_dict(), strict=True)\n",
    "model_new.mask_token = model.mask_token\n",
    "\n",
    "for m in module_list_kd:\n",
    "    getattr(model_new, m).load_state_dict(getattr(model_kd, m).state_dict(), strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(model_1, model_2):\n",
    "    models_differ = 0\n",
    "    for key_item_1, key_item_2 in zip(model_1.state_dict().items(), model_2.state_dict().items()):\n",
    "        if torch.equal(key_item_1[1], key_item_2[1]):\n",
    "            pass\n",
    "        else:\n",
    "            models_differ += 1\n",
    "            if (key_item_1[0] == key_item_2[0]):\n",
    "                print('Mismatch found at', key_item_1[0], key_item_2[0])\n",
    "            else:\n",
    "                print('Error at', key_item_1[0], key_item_2[0])\n",
    "            return False\n",
    "    if models_differ == 0:\n",
    "        # print('Models match perfectly! :)')\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m, k in zip(model_kd.named_children(), model_new.named_children()):\n",
    "    print(m[0], compare_models(m[1],k[1]), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [x for x in model_new.named_children() if x[0] != 'adapter']\n",
    "for m, k in zip(model.named_children(), l):\n",
    "    print(m[0], compare_models(m[1],k[1]), '\\n')\n",
    "    # except:\n",
    "    #     print(m[0], \"Size Mismatch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_new.state_dict(), CKPT_NEW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_new = torch.load(CKPT_NEW)\n",
    "print(model_kd.load_state_dict(ckpt_new, strict=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_enc_dec(model_str, device):\n",
    "    teacher = load_model(\"checkpoints/DUSt3R_ViTLarge_BaseDecoder_512_dpt.pth\", device)\n",
    "    teacher.eval()\n",
    "\n",
    "    model = eval(model_str)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    module_list = ['decoder_embed', 'dec_norm', 'dec_blocks', 'dec_norm', 'dec_blocks2', 'downstream_head1', 'downstream_head2']\n",
    "    for m in module_list:\n",
    "        getattr(model, m).load_state_dict(getattr(teacher, m).state_dict(), strict=True)\n",
    "\n",
    "    return teacher, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher, model = build_model_enc_dec(MODEL_NEW, 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [x for x in model.named_children() if x[0] != 'adapter']\n",
    "for m, k in zip(teacher.named_children(), l):\n",
    "    print(m[0], compare_models(m[1],k[1]), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open('/ssd1/sa58728/dust3r/data/co3d_subset_processed/apple/110_13051_23361/images/frame000060.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open('/ssd1/sa58728/dust3r/data/co3d_subset_processed/apple/110_13051_23361/images/frame000030.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = torch.load('log/train_w_0/checkpoint-1.pth', map_location='cpu')['model']\n",
    "w1 = torch.load('log/train_x_1/checkpoint-2.pth', map_location='cpu')['model']\n",
    "w2 = torch.load('checkpoints/DUSt3R_ViTLarge_BaseDecoder_512_dpt.pth', map_location='cpu')['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h0 = torch.load('log_kd/log_h/ckpt/iter_2475.pth', map_location='cpu')\n",
    "h1 = torch.load('log_kd/log_h/ckpt/iter_4950.pth', map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, j in zip(w2.items(), w0.items()):\n",
    "    if torch.equal(i[1], j[1]):\n",
    "        print(i[0], torch.equal(i[1], j[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0['model']['mask_token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "w = torch.load('log/train_10/checkpoint-last.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w['best_so_far']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Habitat MP3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# i = cv2.imread('/ssd1/wenyan/scannetpp_processed/d6d9ddb03f/depths/DSC05114.png')\n",
    "i = cv2.imread('/mnt/wenyan/scannetpp_processed/d6d9ddb03f/depths/DSC05114.png')\n",
    "i = i * 255.0 / i.max()\n",
    "plt.imshow(i / 255.0, cmap='inferno')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i.min(), i.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ScanNet++ Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = '/ssd1/wenyan/scannetpp_processed'\n",
    "\n",
    "def split_train_test(scenes, split=0.8):\n",
    "    n = len(scenes)\n",
    "    train = scenes[:int(n * split)]\n",
    "    test = scenes[int(n * split):]\n",
    "    return train, test\n",
    "\n",
    "def get_scene_frames(scene, base_path):\n",
    "    frames = os.listdir(os.path.join(base_path, scene, 'images'))\n",
    "    return [int(f.split('.')[0].split(\"DSC\")[1].lstrip(\"0\")) for f in frames if f.endswith('.JPG')]\n",
    "\n",
    "\n",
    "def get_split_scenes(split):\n",
    "    scenes = {}\n",
    "    for scene in split:\n",
    "        if os.path.isdir(os.path.join(BASE_PATH, scene)):\n",
    "            frames = get_scene_frames(scene, BASE_PATH)\n",
    "            scenes[scene] = frames\n",
    "    return scenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenes = os.listdir(BASE_PATH)\n",
    "train, test = split_train_test(scenes)\n",
    "train = get_split_scenes(train)\n",
    "test = get_split_scenes(test)\n",
    "\n",
    "with open('selected_seqs_train.json', 'w') as file:\n",
    "    json.dump(train, file)\n",
    "with open('selected_seqs_test.json', 'w') as file:\n",
    "    json.dump(test, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dust3r.datasets import ScanNet\n",
    "\n",
    "dataset = ScanNet(split='train', ROOT='/ssd1/wenyan/scannetpp_processed', aug_crop=16, mask_bg='rand', resolution=224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dataset:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL3DV Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = '/ssd1/sa58728/dust3r/data/DL3DV-10K/'\n",
    "\n",
    "def split_train_test(scenes, split=0.8):\n",
    "    n = len(scenes)\n",
    "    train = scenes[:int(n * split)]\n",
    "    test = scenes[int(n * split):]\n",
    "    return train, test\n",
    "\n",
    "def get_scene_frames(scene, base_path):\n",
    "    frames = os.listdir(os.path.join(base_path, scene, 'gaussian_splat/images_4'))\n",
    "    return [int(f.split('.')[0].split(\"frame_\")[1].lstrip(\"0\")) for f in frames if f.endswith('.png')]\n",
    "\n",
    "\n",
    "def get_split_scenes(split):\n",
    "    scenes = {}\n",
    "    for scene in split:\n",
    "        if os.path.isdir(os.path.join(BASE_PATH, scene)):\n",
    "            frames = get_scene_frames(scene, BASE_PATH)\n",
    "            scenes[scene] = frames\n",
    "    return scenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenes = os.listdir(BASE_PATH)\n",
    "train, test = split_train_test(scenes)\n",
    "train = get_split_scenes(train)\n",
    "test = get_split_scenes(test)\n",
    "\n",
    "with open('selected_seqs_train.json', 'w') as file:\n",
    "    json.dump(train, file)\n",
    "with open('selected_seqs_test.json', 'w') as file:\n",
    "    json.dump(test, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dust3r.datasets import DL3DV\n",
    "\n",
    "dataset = DL3DV(split='train', ROOT='/ssd1/sa58728/dust3r/data/DL3DV-10K', aug_crop=16, mask_bg='rand', resolution=224)\n",
    "\n",
    "for i in dataset:\n",
    "    print(i[0]['img'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(i[0]['img'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MegaDepth Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = '/ssd1/sa58728/dust3r/data/MegaDepth_v1/'\n",
    "\n",
    "def split_train_test(scenes, split=0.8):\n",
    "    n = len(scenes)\n",
    "    train = scenes[:int(n * split)]\n",
    "    test = scenes[int(n * split):]\n",
    "    return train, test\n",
    "\n",
    "def get_scene_frames(scene, base_path):\n",
    "    if scene.endswith('list'):\n",
    "        return []\n",
    "    try:\n",
    "        frames = os.listdir(os.path.join(base_path, scene, 'dense0/imgs/'))\n",
    "    except:\n",
    "        frames = os.listdir(os.path.join(base_path, scene, 'dense1/imgs/'))\n",
    "    return [f.split('.')[0] for f in frames if f.endswith('.jpg')]\n",
    "\n",
    "\n",
    "def get_split_scenes(split):\n",
    "    scenes = {}\n",
    "    for scene in split:\n",
    "        if os.path.isdir(os.path.join(BASE_PATH, scene)):\n",
    "            frames = get_scene_frames(scene, BASE_PATH)\n",
    "            scenes[scene] = frames\n",
    "    return scenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenes = os.listdir(BASE_PATH)\n",
    "train, test = split_train_test(scenes)\n",
    "train = get_split_scenes(train)\n",
    "test = get_split_scenes(test)\n",
    "\n",
    "with open('selected_seqs_train.json', 'w') as file:\n",
    "    json.dump(train, file)\n",
    "with open('selected_seqs_test.json', 'w') as file:\n",
    "    json.dump(test, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dust3r.datasets import MegaDepth\n",
    "\n",
    "dataset = MegaDepth(split='train', ROOT='/ssd1/sa58728/dust3r/data/MegaDepth_v1', aug_crop=16, mask_bg='rand', resolution=224)\n",
    "\n",
    "for i in dataset:\n",
    "    print(i[0]['img'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(i[0]['img'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEAN = 3\n",
    "STD = 9\n",
    "\n",
    "m, n = [], []\n",
    "for i in range(100000):\n",
    "    m.append(int(rng.normal(loc=0.0, scale=STD)) + MEAN)\n",
    "    n.append(int(rng.normal(loc=0.0, scale=STD)))\n",
    "o = [m - n for m, n in zip(m, n)] + [n - m for m, n in zip(m, n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v2\n",
    "bins = plt.hist(o, bins=range(-40,6), align='left', density=True, cumulative=False)\n",
    "# plt.xticks(np.arange(-21, 22, 3))\n",
    "# plt.yticks(np.arange(0, 1.1, 0.05))\n",
    "# plt.grid(True, alpha=0.3)\n",
    "# plt.hlines(0.025, -20, 22, color='r', alpha=0.5)\n",
    "# plt.hlines(0.975, -20, 22, color='r', alpha=0.5)\n",
    "# plt.hlines(0.25, -20, 22, color='r', alpha=0.5)\n",
    "# plt.hlines(0.75, -20, 22, color='r', alpha=0.5)\n",
    "# plt.vlines(-9, 0, 1, color='r', alpha=0.5)\n",
    "# plt.vlines(-3, 0, 1, color='r', alpha=0.5)\n",
    "# plt.vlines(3, 0, 1, color='r', alpha=0.5)\n",
    "# plt.vlines(9, 0, 1, color='r', alpha=0.5)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = plt.hist(o, bins=range(-40,10), align='left', density=True, cumulative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "MEAN = 3\n",
    "STD = 1\n",
    "last = 10000\n",
    "o = []\n",
    "\n",
    "combinations = [\n",
    "    (i, j)\n",
    "    for i, j in itertools.combinations(range(100), 2)\n",
    "    if abs(i-j) == 3\n",
    "]\n",
    "# combinations = [(i, j)\n",
    "#                     for i, j in itertools.combinations(range(100), 2)\n",
    "#                     if 0 < abs(i-j) <= 30 and abs(i-j) % 5 == 0\n",
    "#                     ]\n",
    "\n",
    "for idx in range(last):\n",
    "    im1_idx, im2_idx = combinations[idx % len(combinations)]\n",
    "    imgs_idxs = [max(0, min(im_idx + int(rng.normal(loc=0.0, scale=STD)), last)) for im_idx in [im2_idx, im1_idx]]\n",
    "    # imgs_idxs = [max(0, min(im_idx + rng.integers(-4, 5), last)) for im_idx in [im2_idx, im1_idx]]\n",
    "    o.append(imgs_idxs[1] - imgs_idxs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v1\n",
    "bins = plt.hist(o, bins=range(-24,30), align='left', density=True, cumulative=True)\n",
    "plt.xticks(np.arange(-20, 22, 5))\n",
    "plt.yticks(np.arange(0, 1.1, 0.05))\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.hlines(0.025, -20, 22, color='r', alpha=0.5)\n",
    "plt.hlines(0.975, -20, 22, color='r', alpha=0.5)\n",
    "plt.hlines(0.25, -20, 22, color='r', alpha=0.5)\n",
    "plt.hlines(0.75, -20, 22, color='r', alpha=0.5)\n",
    "plt.vlines(-15, 0, 1, color='r', alpha=0.5)\n",
    "plt.vlines(-5, 0, 1, color='r', alpha=0.5)\n",
    "plt.vlines(15, 0, 1, color='r', alpha=0.5)\n",
    "plt.vlines(5, 0, 1, color='r', alpha=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "combinations = [\n",
    "    (i, j)\n",
    "    for i, j in itertools.combinations(range(1000), 2)\n",
    "    if 0 < abs(i-j) <= 30 and abs(i-j) % 5 == 0\n",
    "]\n",
    "o = []\n",
    "for i, j in combinations:\n",
    "    imgs_idxs = [max(0, min(im_idx + rng.integers(-4, 5), 1000)) for im_idx in [i, j]]\n",
    "    o.append(imgs_idxs[0] - imgs_idxs[1])\n",
    "    o.append(imgs_idxs[1] - imgs_idxs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = plt.hist(o, bins=range(-40,40), align='left', density=True, cumulative=True)\n",
    "plt.xticks(np.arange(-40, 40, 5))\n",
    "plt.yticks(np.arange(0, 1.1, 0.05))\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.hlines(0.025, -40, 40, color='r', alpha=0.5)\n",
    "plt.hlines(0.975, -40, 40, color='r', alpha=0.5)\n",
    "plt.hlines(0.25, -40, 40, color='r', alpha=0.5)\n",
    "plt.hlines(0.75, -40, 40, color='r', alpha=0.5)\n",
    "plt.vlines(-32, 0, 1, color='r', alpha=0.5)\n",
    "plt.vlines(-18, 0, 1, color='r', alpha=0.5)\n",
    "plt.vlines(18, 0, 1, color='r', alpha=0.5)\n",
    "plt.vlines(32, 0, 1, color='r', alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Pair Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dust3r.datasets import get_data_loader  # noqa\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "OUT_FOLDER = \"pairs\"\n",
    "DATASET = \"ScanNet\"\n",
    "GAUSSIAN = True\n",
    "\n",
    "if DATASET == \"Co3D\":\n",
    "    TRAIN_DATA = f\"1000 @ Co3d(split='train', ROOT='/ssd1/wenyan/co3d_2_cat_processed', aug_crop=16, mask_bg='rand', resolution=224, transform=ImgNorm, gaussian_frames={GAUSSIAN})\"\n",
    "elif DATASET == \"ScanNet\":\n",
    "    TRAIN_DATA = f\"1000 @ ScanNetpp(split='train', ROOT='/mnt/vita-nas/scannetpp_processed', aug_crop=16, resolution=224, transform=ImgNorm)\"\n",
    "elif DATASET == \"DL3DV\":\n",
    "    TRAIN_DATA = f\"1000 @ DL3DV(split='train', ROOT='/ssd1/sa58728/dust3r/data/DL3DV-10K', aug_crop=16, mask_bg='rand', resolution=224, transform=ImgNorm, gaussian_frames={GAUSSIAN})\"\n",
    "\n",
    "loader = get_data_loader(\n",
    "    TRAIN_DATA,\n",
    "    batch_size=1,\n",
    "    num_workers=4,\n",
    "    pin_mem=True,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "if hasattr(loader, 'dataset') and hasattr(loader.dataset, 'set_epoch'):\n",
    "    loader.dataset.set_epoch(0)\n",
    "if hasattr(loader, 'sampler') and hasattr(loader.sampler, 'set_epoch'):\n",
    "    loader.sampler.set_epoch(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image_pair(pair, plot=False):\n",
    "    name1 = pair[0]['instance'][0].split('.')[0]\n",
    "    name2 = pair[1]['instance'][0].split('.')[0]\n",
    "    name = f\"{name1}_{name2}\"\n",
    "    os.makedirs(f\"{OUT_FOLDER}/{DATASET}/{name}\", exist_ok=True)\n",
    "\n",
    "    img1 = ((pair[0]['img'][0] * 0.5) + 0.5)\n",
    "    img2 = ((pair[1]['img'][0] * 0.5) + 0.5)\n",
    "\n",
    "    save_image(img1, f\"{OUT_FOLDER}/{DATASET}/{name}/{name1}.png\")\n",
    "    save_image(img2, f\"{OUT_FOLDER}/{DATASET}/{name}/{name2}.png\")\n",
    "\n",
    "    if plot:\n",
    "        plt.imshow(img1.permute(1, 2, 0))\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "        plt.imshow(img2.permute(1, 2, 0))\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, pair in enumerate(loader, 1):\n",
    "    save_image_pair(pair)\n",
    "    if i == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "Image.open(\"/mnt/vita-nas/scannetpp_processed/116456116b/images/DSC09535.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "with np.load(os.path.join('/mnt/vita-nas/scannetpp_processed', 'all_metadata.npz')) as data:\n",
    "    scenes = data['scenes']\n",
    "    sceneids = data['sceneids']\n",
    "    images = data['images']\n",
    "    intrinsics = data['intrinsics'].astype(np.float32)\n",
    "    trajectories = data['trajectories'].astype(np.float32)\n",
    "    pairs = data['pairs'][:, :2].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RoMa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "# RoMa\n",
    "import torch.nn.functional as F\n",
    "from RoMa.roma.utils.utils import tensor_to_pil\n",
    "from RoMa.roma import roma_outdoor\n",
    "# DUSt3R\n",
    "# torch.backends.cuda.matmul.allow_tf32 = True  # for gpu >= Ampere and pytorch >= 1.12\n",
    "from dust3r.losses import *  # noqa: F401, needed when loading the model\n",
    "from dust3r.inference import loss_of_one_batch, load_model\n",
    "from dust3r.utils.image import load_images\n",
    "from dust3r.image_pairs import make_pairs\n",
    "from dust3r.utils.device import to_cpu, collate_with_cat\n",
    "from misc.test_pairs import *\n",
    "\n",
    "CKPT = \"checkpoints/DUSt3R_ViTLarge_BaseDecoder_512_dpt.pth\"\n",
    "MODEL_KD = \"AsymmetricCroCo3DStereo(pos_embed='RoPE100', img_size=(224, 224), head_type='dpt', \\\n",
    "            output_mode='pts3d', depth_mode=('exp', -inf, inf), conf_mode=('exp', 1, inf), \\\n",
    "            enc_embed_dim=384, enc_depth=12, enc_num_heads=6, dec_embed_dim=768, dec_depth=12, dec_num_heads=12, adapter=True)\"\n",
    "CKPT_KD = \"log/train_2/checkpoint-best.pth\"\n",
    "TEST_CRITERION = \"ConfLoss(Regr3D(L21, norm_mode='avg_dis', kd=True), alpha=0.2) + Regr3D_ScaleShiftInv(L21, gt_scale=True, kd=True)\"\n",
    "\n",
    "device = torch.device('cpu')\n",
    "device = torch.device('cuda:6' if torch.cuda.is_available() else 'cpu')\n",
    "img_path = \"test/RoMa/\"\n",
    "im1_path = img_path + \"DSC00410.png\"\n",
    "im2_path = img_path + \"DSC09985.png\"\n",
    "save_path = \"./roma.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "roma_model = roma_outdoor(device=device, coarse_res=224, upsample_res=224)\n",
    "H, W = roma_model.get_output_resolution()\n",
    "\n",
    "im1 = Image.open(im1_path).resize((W, H))\n",
    "im2 = Image.open(im2_path).resize((W, H))\n",
    "\n",
    "# teacher, model = load_pretrained(MODEL_KD, CKPT, CKPT_KD, device)\n",
    "test_criterion = eval(TEST_CRITERION).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = load_images(img_path, size=224)\n",
    "pairs = make_pairs(images, scene_graph='complete', prefilter=None, symmetrize=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    result = loss_of_one_batch(collate_with_cat(pairs), teacher, test_criterion, device,\n",
    "                            symmetrize_batch=False, features=True,\n",
    "                            kd=True, kd_out=True, teacher=teacher, lmd=10)\n",
    "    \n",
    "result = to_cpu(result)\n",
    "loss_value, loss_details = result['loss']  # criterion returns two values\n",
    "loss_details['loss'] = loss_value.item()\n",
    "print(f\"Details: {loss_details}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['pred1']['pts3d'].shape, result['pred2']['pts3d_in_other_view'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match\n",
    "warp, certainty = roma_model.match(im1_path, im2_path, device=device)\n",
    "warp, certainty = warp.reshape(-1, 2*H*W, 4), certainty.reshape(-1, 2*H*W)\n",
    "warp_filter = warp[certainty > 0.55]\n",
    "# out, valid = roma_model.sample(warp, certainty)\n",
    "kptsA, kptsB = roma_model.to_pixel_coordinates(warp_filter, H, W, H, W)\n",
    "kptsA, kptsB = kptsA.cpu().numpy().astype('int'), kptsB.cpu().numpy().astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "cert = certainty.reshape(H,2*W).cpu() > 0.1\n",
    "im = ax.imshow(cert)\n",
    "plt.axis('off')\n",
    "# cax = fig.add_axes([ax.get_position().x1+0.01,ax.get_position().y0,0.02,ax.get_position().height])\n",
    "# plt.colorbar(im, cax=cax) # Similar to fig.colorbar(im, cax = cax)\n",
    "plt.show()\n",
    "print(cert.float().mean()*100, cert.float().mean()*H*W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(certainty.cpu(), bins=100, log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(certainty.cpu().numpy().flatten(), cumulative=True, bins=100)\n",
    "plt.hlines(90000, 0, 1, color='r', alpha=0.5)\n",
    "plt.vlines(0.55, 0, 100000, color='r', alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warp.min(), warp.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kptsA, kptsB = roma_model.to_pixel_coordinates(warp, H, W, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kptsA.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling not needed, but can be done with model.sample(warp, certainty)\n",
    "x1 = (torch.tensor(np.array(im1)) / 255).to(device).permute(2, 0, 1)\n",
    "x2 = (torch.tensor(np.array(im2)) / 255).to(device).permute(2, 0, 1)\n",
    "\n",
    "# take only the matching part from images and copy colors (xyz values once normalized)\n",
    "im1_transfer_rgb = F.grid_sample(\n",
    "    x1[None], warp[:, W:, :2][None], mode=\"bilinear\", align_corners=False\n",
    ")[0]\n",
    "im2_transfer_rgb = F.grid_sample(\n",
    "    x2[None], warp[:,:W, 2:][None], mode=\"bilinear\", align_corners=False\n",
    ")[0]\n",
    "warp_im = torch.cat((im2_transfer_rgb,im1_transfer_rgb),dim=2)\n",
    "white_im = torch.ones((H,2*W),device=device)\n",
    "vis_im = certainty * warp_im + (1 - certainty) * white_im\n",
    "tensor_to_pil(vis_im, unnormalize=False).save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kptsB.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 2500\n",
    "print(kptsA[i])\n",
    "plt.imshow(im1)\n",
    "plt.scatter(*kptsA[i], c='r', s=10)\n",
    "plt.show()\n",
    "plt.imshow(im2)\n",
    "plt.scatter(*kptsB[i], c='r', s=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1240\n",
    "print(kptsA[i])\n",
    "plt.imshow((pairs[0][1]['img'][0].permute(1, 2, 0)*0.5)+0.5)\n",
    "plt.scatter(*kptsA[i], c='r', s=10)\n",
    "plt.show()\n",
    "plt.imshow((pairs[0][0]['img'][0].permute(1, 2, 0)*0.5)+0.5)\n",
    "plt.scatter(*kptsB[i], c='r', s=10)\n",
    "plt.show()\n",
    "plt.imshow(result['pred2']['pts3d_in_other_view'][0,:,:,-1])\n",
    "plt.scatter(*kptsA[i], c='r', s=10)\n",
    "plt.show()\n",
    "plt.imshow(result['pred1']['pts3d'][0,:,:,-1])\n",
    "plt.scatter(*kptsB[i], c='r', s=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['pred1']['pts3d'][0,kptsB[:,0],kptsB[:,1]].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = result['pred1']['pts3d'][0,kptsB[:,0],kptsB[:,1]] # * valid[:,None].cpu()\n",
    "p2 = result['pred2']['pts3d_in_other_view'][0,kptsA[:,0],kptsA[:,1]] # * valid[:,None].cpu()\n",
    "print(p1.shape, p2.shape, (p1 - p2).abs().mean(), ((p1 - p2)**2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kptsA[i].astype('int'), kptsA[i], kptsB[i].astype('int'), kptsB[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['pred1']['pts3d'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch RoMa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from dust3r.inference import *\n",
    "from test_kd import *\n",
    "\n",
    "H, W = 224, 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import Sized\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "torch.backends.cuda.matmul.allow_tf32 = True  # for gpu >= Ampere and pytorch >= 1.12\n",
    "\n",
    "from dust3r.model import AsymmetricCroCo3DStereo, inf  # noqa: F401, needed when loading the model\n",
    "from dust3r.datasets import get_data_loader  # noqa\n",
    "from dust3r.losses import *  # noqa: F401, needed when loading the model\n",
    "from dust3r.inference import loss_of_one_batch, load_model\n",
    "import dust3r.utils.path_to_croco  # noqa: F401\n",
    "import croco.utils.misc as misc  # noqa\n",
    "from RoMa.roma import roma_outdoor\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "TEST_DATA = \"Co3d(split='test', ROOT='/ssd1/sa58728/dust3r/data/co3d_subset_processed', resolution=224, seed=777, gaussian_frames=False)\" # Unseen scenes\n",
    "TEST_DATA += \" + ScanNet(split='test', ROOT='/ssd1/wenyan/scannetpp_processed', resolution=224, seed=777, gaussian_frames=True)\" # Unseen scenes\n",
    "TEST_DATA += \" + DL3DV(split='test', ROOT='/ssd1/sa58728/dust3r/data/DL3DV-10K', resolution=224, seed=777, gaussian_frames=True)\" # Unseen scenes\n",
    "\n",
    "MODEL_KD = \"AsymmetricCroCo3DStereo(pos_embed='RoPE100', img_size=(224, 224), head_type='dpt', \\\n",
    "            output_mode='pts3d', depth_mode=('exp', -inf, inf), conf_mode=('exp', 1, inf), \\\n",
    "            enc_embed_dim=384, enc_depth=12, enc_num_heads=6, dec_embed_dim=768, dec_depth=12, dec_num_heads=12, adapter=True)\"\n",
    "CKPT = \"checkpoints/DUSt3R_ViTLarge_BaseDecoder_512_dpt.pth\"\n",
    "CKPT_KD = None # \"checkpoints/DUSt3R_ViTSmall_BaseDecoder_512_dpt_kd.pth\"\n",
    "TEST_CRITERION = \"ConfLoss(Regr3D(L21, norm_mode='avg_dis', kd=True), alpha=0.2) + Regr3D_ScaleShiftInv(L21, gt_scale=True, kd=True)\"\n",
    "\n",
    "def get_args_parser():\n",
    "    parser = argparse.ArgumentParser('DUSt3R training', add_help=False)\n",
    "    # model and criterion\n",
    "    parser.add_argument('--model', default=MODEL_KD, type=str, help=\"string containing the model to build\")\n",
    "    parser.add_argument('--pretrained', default=None, help='path of a starting checkpoint') # CKPT_KD\n",
    "    parser.add_argument('--test_criterion', default=TEST_CRITERION, type=str, help=\"test criterion\")\n",
    "    # dataset\n",
    "    parser.add_argument('--test_dataset', default=TEST_DATA, type=str, help=\"testing set\")\n",
    "    parser.add_argument('--seed', default=777, type=int, help=\"Random seed\")\n",
    "    # others\n",
    "    parser.add_argument('--num_workers', default=8, type=int)\n",
    "    parser.add_argument('--world_size', default=1, type=int, help='number of distributed processes')\n",
    "    parser.add_argument('--local_rank', default=0, type=int)\n",
    "    parser.add_argument('--dist_url', default='env://', help='url used to set up distributed training')\n",
    "\n",
    "    parser.add_argument('--eval_freq', type=int, default=1, help='Test loss evaluation frequency')\n",
    "    parser.add_argument('--save_freq', default=1, type=int,\n",
    "                        help='frequence (number of epochs) to save checkpoint in checkpoint-last.pth')\n",
    "    parser.add_argument('--keep_freq', default=1, type=int,\n",
    "                        help='frequence (number of epochs) to save checkpoint in checkpoint-%d.pth')\n",
    "    parser.add_argument('--print_freq', default=1000, type=int,\n",
    "                        help='frequence (number of iterations) to print infos while training')\n",
    "    parser.add_argument('--teacher_path', default=CKPT, type=str, help=\"path to the teacher model\")\n",
    "\n",
    "    parser.add_argument('--lmd', default=10, type=float, help=\"kd loss weight\")\n",
    "    parser.add_argument('--output_dir', default='./log/train/', type=str, help=\"path where to save the output\")\n",
    "    parser.add_argument('--cuda', default=7, type=int, help=\"cuda device\")\n",
    "    parser.add_argument('--ckpt', default=None, type=str, help=\"resume from checkpoint\") # \"log/ckpt/iter_24750.pth\"\n",
    "    parser.add_argument('--batch_size', default=8, type=int, help=\"Batch size per GPU (effective batch size is batch_size * accum_iter * # gpus\")\n",
    "    parser.add_argument('--accum_iter', default=1, type=int, help=\"Accumulate gradient iterations\")\n",
    "    parser.add_argument('--kd', default=True, type=bool)\n",
    "    parser.add_argument('--kd_out', default=True, action='store_true', help=\"knowledge distillation (output)\")\n",
    "\n",
    "    return parser\n",
    "\n",
    "def build_dataset(dataset, batch_size, num_workers, test=False):\n",
    "    split = ['Train', 'Test'][test]\n",
    "    print(f'Building {split} Data loader for dataset: ', dataset)\n",
    "    loader = get_data_loader(dataset,\n",
    "                             batch_size=batch_size,\n",
    "                             num_workers=num_workers,\n",
    "                             pin_mem=True,\n",
    "                             shuffle=not (test),\n",
    "                             drop_last=not (test))\n",
    "\n",
    "    print(f\"{split} dataset length: \", len(loader))\n",
    "    return loader\n",
    "\n",
    "def load_pretrained(args, device):\n",
    "    teacher = load_model(args.teacher_path, device)\n",
    "    teacher.eval()\n",
    "\n",
    "    model = deepcopy(teacher)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    model_kd = eval(args.model)\n",
    "    model_kd.to(device)\n",
    "    model_kd.eval()\n",
    "\n",
    "    ckpt = torch.load(args.ckpt, map_location=device)\n",
    "    try:\n",
    "        print(model_kd.load_state_dict(ckpt['model'], strict=True))\n",
    "        args.start_epoch = ckpt['epoch']\n",
    "    except:\n",
    "        print(model_kd.load_state_dict(ckpt, strict=True))\n",
    "    del ckpt  # in case it occupies memory\n",
    "\n",
    "    model.patch_embed = deepcopy(model_kd.patch_embed)\n",
    "    model.mask_generator = deepcopy(model_kd.mask_generator)\n",
    "    model.rope = deepcopy(model_kd.rope)\n",
    "    model.enc_blocks = deepcopy(model_kd.enc_blocks)\n",
    "    model.enc_norm = deepcopy(model_kd.enc_norm)\n",
    "    model.adapter = deepcopy(model_kd.adapter)\n",
    "\n",
    "    return teacher, model\n",
    "\n",
    "def build_model_enc_dec(model_str, device, args):\n",
    "    teacher = load_model(\"checkpoints/DUSt3R_ViTLarge_BaseDecoder_512_dpt.pth\", device)\n",
    "    teacher.eval()\n",
    "\n",
    "    if \"x\" in args.output_dir:\n",
    "        print(\"Using pretrained Dust3R\")\n",
    "        model = load_model(\"checkpoints/DUSt3R_ViTLarge_BaseDecoder_512_dpt.pth\", device)\n",
    "    else:\n",
    "        print(\"Training from scratch\")\n",
    "        model = eval(model_str)\n",
    "\n",
    "    model.to(device)\n",
    "    if args.ckpt:\n",
    "        ckpt = torch.load(args.ckpt)\n",
    "        print(model.load_state_dict(ckpt['model'], strict=True))\n",
    "        args.start_epoch = ckpt['epoch']\n",
    "        model.train()\n",
    "\n",
    "    module_list = ['decoder_embed', 'dec_blocks', 'dec_norm', 'dec_blocks2', 'downstream_head1', 'downstream_head2']\n",
    "    for m in module_list:\n",
    "        getattr(model, m).load_state_dict(getattr(teacher, m).state_dict(), strict=True)\n",
    "        getattr(model, m).eval()\n",
    "    model.mask_token = teacher.mask_token\n",
    "\n",
    "    return teacher, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = get_args_parser()\n",
    "args = args.parse_args('')\n",
    "\n",
    "misc.init_distributed_mode(args)\n",
    "\n",
    "device = f\"cuda:6\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device)\n",
    "seed = args.seed\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "cudnn.benchmark = False\n",
    "cudnn.deterministic = True\n",
    "\n",
    "data_loader_test = {dataset.split('(')[0]: build_dataset(dataset, args.batch_size, args.num_workers, test=True)\n",
    "                    for dataset in args.test_dataset.split('+')}\n",
    "\n",
    "# teacher, model = load_pretrained(args, device)\n",
    "teacher, model = build_model_enc_dec(args.model, device, args)\n",
    "roma_model = roma_outdoor(device=device, coarse_res=224, upsample_res=224)\n",
    "test_criterion = eval(args.test_criterion or args.criterion).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_plot(tensor, norm=None):\n",
    "    STD = np.array([0.229, 0.224, 0.225])\n",
    "    MEAN = np.array([0.485, 0.456, 0.406])\n",
    "    if norm == 'roma':\n",
    "        return tensor.cpu().permute(1,2,0).numpy() * STD + MEAN\n",
    "    elif norm == 'dust3r':\n",
    "        return (tensor.cpu().permute(1,2,0).numpy() * 0.5) + 0.5\n",
    "    return tensor.cpu().permute(1,2,0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def loss_of_one_batch(batch, model, criterion, device, symmetrize_batch=False, use_amp=False, ret=None, \n",
    "#                       return_times=False, features_only=False, features=False, \n",
    "#                       kd=False, kd_out=False, teacher=None, lmd=1, criterion_kd=torch.nn.MSELoss(),\n",
    "#                       roma_model=None):\n",
    "#     view1, view2 = batch\n",
    "#     for view in batch:\n",
    "#         for name in 'img pts3d valid_mask camera_pose camera_intrinsics F_matrix corres'.split():  # pseudo_focal\n",
    "#             if name not in view:\n",
    "#                 continue\n",
    "#             view[name] = view[name].to(device, non_blocking=True)\n",
    "\n",
    "#     with torch.cuda.amp.autocast(enabled=bool(use_amp)):\n",
    "#         outs = model(view1, view2, return_times=return_times, features_only=features_only, features=features)\n",
    "#         pred1, pred2 = outs\n",
    "\n",
    "#     loss, loss_tot = 0, 0\n",
    "#     loss_dict = {}\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         renorm_img1 = (view1['img'] * 0.5 + 0.5 - ROMA_MEAN.to(device)) / ROMA_STD.to(device)\n",
    "#         renorm_img2 = (view2['img'] * 0.5 + 0.5 - ROMA_MEAN.to(device)) / ROMA_STD.to(device)\n",
    "#         warp, certainty = roma_model.match(renorm_img1, renorm_img2, batched=True, device=device)\n",
    "#         out, valid = roma_model.sample(warp, certainty) # THIS DOES NOT WORK WITH BATCHES\n",
    "#         warp, certainty = warp.reshape(-1, 2*SIZE*SIZE, 4), certainty.reshape(-1, 2*SIZE*SIZE)\n",
    "#         idxs = torch.topk(certainty, 1000, dim=-1).indices\n",
    "#         certainty = certainty.gather(-1, idxs)\n",
    "#         warp = warp.gather(-2, idxs.unsqueeze(-1).expand(-1, -1, 4))\n",
    "#         kptsA, kptsB = roma_model.to_pixel_coordinates(warp, SIZE, SIZE, SIZE, SIZE)\n",
    "#         kptsA, kptsB = kptsA.type(torch.int64), kptsB.type(torch.int64)\n",
    "\n",
    "#     p1 = outs[0]['pts3d'].gather(1, kptsB.unsqueeze(-1).expand(-1, -1, -1, 3))\n",
    "#     p2 = outs[1]['pts3d_in_other_view'].gather(1, kptsA.unsqueeze(-1).expand(-1, -1, -1, 3))\n",
    "\n",
    "#     loss_dict['roma_mse'] = ((p1 - p2)**2).mean()\n",
    "#     loss_dict['roma_mae'] = ((p1 - p2).abs()).mean()\n",
    "#     loss_tot += loss_dict['roma_mse'] * lmd\n",
    "#     loss = (loss_tot, loss_dict)\n",
    "\n",
    "#     result = dict(view1=view1, view2=view2, pred1=pred1, pred2=pred2, loss=loss)\n",
    "#     return result[ret] if ret else result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @torch.no_grad()\n",
    "# def test_one_epoch(model: torch.nn.Module, criterion: torch.nn.Module,\n",
    "#                    data_loader: Sized, device: torch.device, epoch: int,\n",
    "#                    args, log_writer=None, prefix='test', \n",
    "#                    kd=False, teacher=None, features=False, curr_step=0,\n",
    "#                    roma_model=None):\n",
    "                    \n",
    "#     model.eval()\n",
    "#     metric_logger = misc.MetricLogger(delimiter=\"  \")\n",
    "#     metric_logger.meters = defaultdict(lambda: misc.SmoothedValue(window_size=9**9))\n",
    "#     header = 'Test Epoch: [{}]\\n>'.format(epoch)\n",
    "\n",
    "#     if hasattr(data_loader, 'dataset') and hasattr(data_loader.dataset, 'set_epoch'):\n",
    "#         data_loader.dataset.set_epoch(epoch)\n",
    "#     if hasattr(data_loader, 'sampler') and hasattr(data_loader.sampler, 'set_epoch'):\n",
    "#         data_loader.sampler.set_epoch(epoch)\n",
    "\n",
    "#     for _, batch in enumerate(metric_logger.log_every(data_loader, args.print_freq, header)):\n",
    "        \n",
    "#         loss_tuple = loss_of_one_batch(batch, model, criterion, device,\n",
    "#                                        symmetrize_batch=True, features=features, ret='loss', \n",
    "#                                        kd=kd, kd_out=args.kd_out, teacher=teacher, lmd=args.lmd,\n",
    "#                                        roma_model=roma_model)\n",
    "#         return loss_tuple        \n",
    "\n",
    "#     # gather the stats from all processes\n",
    "#     metric_logger.synchronize_between_processes()\n",
    "#     print(\"Averaged stats:\", metric_logger)\n",
    "\n",
    "#     aggs = [('avg', 'global_avg'), ('med', 'median')]\n",
    "#     results = {f'{k}_{tag}': getattr(meter, attr) for k, meter in metric_logger.meters.items() for tag, attr in aggs}\n",
    "\n",
    "#     return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_name, testset in reversed(data_loader_test.items()):\n",
    "    print(test_name)\n",
    "    model.eval()\n",
    "    metric_logger = misc.MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.meters = defaultdict(lambda: misc.SmoothedValue(window_size=9**9))\n",
    "    header = 'Test Epoch: [{}]\\n>'.format(0)\n",
    "\n",
    "    if hasattr(testset, 'dataset') and hasattr(testset.dataset, 'set_epoch'):\n",
    "        testset.dataset.set_epoch(0)\n",
    "    if hasattr(testset, 'sampler') and hasattr(testset.sampler, 'set_epoch'):\n",
    "        testset.sampler.set_epoch(0)\n",
    "\n",
    "    for _, batch in enumerate(metric_logger.log_every(testset, args.print_freq, header)):\n",
    "        view1, view2 = batch\n",
    "        for view in batch:\n",
    "            for name in 'img pts3d valid_mask camera_pose camera_intrinsics F_matrix corres'.split():  # pseudo_focal\n",
    "                if name not in view:\n",
    "                    continue\n",
    "                view[name] = view[name].to(device, non_blocking=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outs = teacher(view1, view2, return_times=False, features_only=False, features=True)\n",
    "            pred1, pred2 = outs\n",
    "\n",
    "        loss, loss_tot = 0, 0\n",
    "        loss_dict = {}\n",
    "\n",
    "        renorm_img1 = (view1['img'] * 0.5 + 0.5 - ROMA_MEAN.to(device)) / ROMA_STD.to(device)  \n",
    "        with torch.no_grad():\n",
    "            renorm_img2 = (view2['img'] * 0.5 + 0.5 - ROMA_MEAN.to(device)) / ROMA_STD.to(device)\n",
    "        warp, certainty = roma_model.match(renorm_img1, renorm_img2, batched=True, device=device)\n",
    "        warp, certainty = warp.reshape(-1, 2*H*W, 4), certainty.reshape(-1, 2*H*W)\n",
    "        kptsA, kptsB = roma_model.to_pixel_coordinates(warp, H, W, H, W)\n",
    "        kptsA, kptsB = kptsA.type(torch.int64), kptsB.type(torch.int64)\n",
    "        kptsA, kptsB = kptsA.reshape(-1,H,2*W,2), kptsB.reshape(-1,H,2*W,2)\n",
    "\n",
    "        kpts1 = kptsA[:,:,:W,:] # B, H, W, 2\n",
    "        kpts2 = kptsB[:,:,:W,:] # B, H, W, 2\n",
    "        pred1 = outs[0]['pts3d'] # -> kpts1\n",
    "        pred2 = outs[1]['pts3d_in_other_view'] # -> kpts2\n",
    "        kpts1, kpts2 = kpts1.reshape(-1,H*W,2), kpts2.reshape(-1,H*W,2)\n",
    "        kpts1_flat = torch.from_numpy(np.ravel_multi_index(kpts1.cpu().permute(-1,0,1).numpy(), (H, W), order='F')).to(device)\n",
    "        kpts2_flat = torch.from_numpy(np.ravel_multi_index(kpts2.cpu().permute(-1,0,1).numpy(), (H, W), order='F')).to(device)\n",
    "        pred1_flat = pred1.reshape(-1, H*W, 3)\n",
    "        pred2_flat = pred2.reshape(-1, H*W, 3)\n",
    "        p1 = pred1_flat.gather(1, kpts1_flat.unsqueeze(-1).expand(-1,-1,3))\n",
    "        p2 = pred2_flat.gather(1, kpts2_flat.unsqueeze(-1).expand(-1,-1,3))\n",
    "\n",
    "        cert = (certainty.reshape(-1,H,2*W)[:,:,:W].reshape(-1,H*W) > 0.5).float()\n",
    "        p1c = p1 * cert.unsqueeze(-1)\n",
    "        p2c = p2 * cert.unsqueeze(-1)\n",
    "\n",
    "        loss_dict['roma_mse'] = ((p1c - p2c)**2).mean() / cert.mean()\n",
    "        loss_dict['roma_mae'] = (p1c - p2c).abs().mean() / cert.mean()\n",
    "        loss_tot += loss_dict['roma_mse'] * 10000\n",
    "        loss = (loss_tot, loss_dict)\n",
    "\n",
    "        result = dict(view1=view1, view2=view2, pred1=pred1, pred2=pred2, loss=loss)\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 26545\n",
    "c1 = kptsA.reshape(-1,2*W*H,2)[0,i]\n",
    "print(c1)\n",
    "print(pred1[0, c1[1], c1[0]]) # reverse coordinates\n",
    "plt.imshow(tensor_to_plot(batch[0]['img'][0], norm='dust3r'))\n",
    "plt.scatter(*c1.cpu(), c='r', s=10)\n",
    "plt.show()\n",
    "c2 = kptsB.reshape(-1,2*W*H,2)[0,i]\n",
    "print(c2)\n",
    "print(pred2[0, c2[1], c2[0]]) # reverse coordinates\n",
    "plt.imshow(tensor_to_plot(batch[1]['img'][0], norm='dust3r'))\n",
    "plt.scatter(*c2.cpu(), c='r', s=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 45802\n",
    "c1 = kptsA.reshape(-1,2*W*H,2)[0,i]\n",
    "print(c1)\n",
    "print(pred1[0, c1[1], c1[0]]) # reverse coordinates\n",
    "plt.imshow(pred1[0,...,-1].cpu())\n",
    "plt.scatter(*c1.cpu(), c='r', s=10)\n",
    "plt.show()\n",
    "c2 = kptsB.reshape(-1,2*W*H,2)[0,i]\n",
    "print(c2)\n",
    "print(pred2[0, c2[1], c2[0]]) # reverse coordinates\n",
    "plt.imshow(pred2[0,...,-1].cpu())\n",
    "plt.scatter(*c2.cpu(), c='r', s=10)\n",
    "plt.show()\n",
    "\n",
    "print(pred1[0,*kptsA.reshape(-1,2*H*W,2)[0,i],:].cpu(), pred2[0,*kptsB.reshape(-1,2*H*W,2)[0,i],:].cpu())\n",
    "print(((pred1[0, c1[1], c1[0]] - pred2[0, c2[1], c2[0]])**2).mean())\n",
    "print(((pred1[0, c1[1], c1[0]] - pred2[0, c2[1], c2[0]]).abs()).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cert.reshape(-1,H,W)[0].cpu().numpy(), cmap='inferno')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(warp.reshape(-1,H,2*W,4)[0,...,3].cpu().numpy(), cmap='inferno')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(kptsA[0,...,0].cpu().numpy(), cmap='inferno')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(kptsB[0,...,1].cpu().numpy(), cmap='inferno')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpts1 = kptsA[:,:,:W,:] # B, H, W, 2\n",
    "kpts2 = kptsB[:,:,:W,:] # B, H, W, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpts1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(kpts2[0,...,1].cpu().numpy(), cmap='inferno')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpts1, kpts2 = kpts1.reshape(-1,H*W,2), kpts2.reshape(-1,H*W,2)\n",
    "kpts1_flat = torch.from_numpy(np.ravel_multi_index(kpts1.cpu().permute(-1,0,1).numpy(), (H, W), order='F')).to(device)\n",
    "kpts2_flat = torch.from_numpy(np.ravel_multi_index(kpts2.cpu().permute(-1,0,1).numpy(), (H, W), order='F')).to(device)\n",
    "pred1_flat = pred1.reshape(-1, H*W, 3)\n",
    "pred2_flat = pred2.reshape(-1, H*W, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(kpts2_flat.reshape(-1,H,W)[0,...].cpu().numpy(), cmap='inferno')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpts1_flat.shape, kpts2_flat.shape, pred1_flat.shape, pred2_flat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = pred1_flat.gather(1, kpts1_flat.unsqueeze(-1).expand(-1,-1,3))\n",
    "p2 = pred2_flat.gather(1, kpts2_flat.unsqueeze(-1).expand(-1,-1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((p1 - p2)**2).mean(), (p1 - p2).abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cert = (certainty.reshape(-1,H,2*W)[:,:,:W].reshape(-1,H*W) > 0.5).float()\n",
    "p1c = p1 * cert.unsqueeze(-1)\n",
    "p2c = p2 * cert.unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((p1c - p2c)**2).mean() / cert.mean(), (p1c - p2c).abs().mean() / cert.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.topk(certainty[0], 100).indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 45802\n",
    "c1 = kptsA.reshape(-1,2*W*H,2)[0,i]\n",
    "print(c1)\n",
    "print(p1.reshape(-1,H,W,3)[0, c1[1], c1[0]]) # reverse coordinates\n",
    "plt.imshow(p1.reshape(-1,H,W,3)[0,:,:,-1].cpu().numpy())\n",
    "plt.scatter(*c1.cpu(), c='r', s=10)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "print(p2.reshape(-1,H,W,3)[0, c1[1], c1[0]]) # reverse coordinates\n",
    "plt.imshow(p2.reshape(-1,H,W,3)[0,:,:,-1].cpu().numpy())\n",
    "plt.scatter(*c1.cpu(), c='r', s=10)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "print(pred1[0, c1[1], c1[0]].cpu(), pred2[0, c2[1], c2[0]].cpu())\n",
    "print(((pred1[0, c1[1], c1[0]] - pred2[0, c2[1], c2[0]])**2).mean())\n",
    "print(((pred1[0, c1[1], c1[0]] - pred2[0, c2[1], c2[0]]).abs()).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = p2 - p1\n",
    "plt.imshow(diff.reshape(-1,H,W,3)[0,:,:,2].cpu().numpy())\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff.min(), diff.max(), (diff**2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# RoMa Loss Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "from dust3r.datasets import get_data_loader  # noqa\n",
    "from dust3r.inference import load_model\n",
    "from dust3r.model import AsymmetricCroCo3DStereo, inf  # noqa: F401, needed when loading the model\n",
    "from dust3r.losses import *  # noqa: F401, needed when loading the model\n",
    "from dust3r.inference import loss_of_one_batch\n",
    "import croco.utils.misc as misc  # noqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args_parser():\n",
    "    # others\n",
    "    parser = argparse.ArgumentParser('DUSt3R training', add_help=False)\n",
    "    parser.add_argument('--seed', default=777, type=int, help=\"Random seed\")\n",
    "    parser.add_argument('--num_workers', default=8, type=int)\n",
    "    parser.add_argument('--world_size', default=1, type=int, help='number of distributed processes')\n",
    "    parser.add_argument('--local_rank', default=0, type=int)\n",
    "    parser.add_argument('--dist_url', default='env://', help='url used to set up distributed training')\n",
    "\n",
    "    parser.add_argument('--teacher_ckpt', default=\"checkpoints/DUSt3R_ViTLarge_BaseDecoder_512_dpt.pth\", type=str, help=\"path to the teacher model\")\n",
    "    parser.add_argument('--lmd', default=10, type=float, help=\"kd loss weight\")\n",
    "    parser.add_argument('--cuda', default=0, type=int, help=\"cuda device\")\n",
    "    parser.add_argument('--ckpt', default='/home/sa58728/dust3r/log/gauss_3_roma_1000/checkpoint-best.pth', type=str, help=\"resume from checkpoint\")\n",
    "    parser.add_argument('--batch_size', default=1, type=int, help=\"Batch size per GPU (effective batch size is batch_size * accum_iter * # gpus\")\n",
    "    parser.add_argument('--kd_enc', default=True, type=bool)\n",
    "    parser.add_argument('--kd_out', default=True, action='store_true', help=\"knowledge distillation (output)\")\n",
    "    parser.add_argument('--roma', default=1, action='store_true', help=\"Use RoMa\")\n",
    "    parser.add_argument('--encoder_only', default=True, action='store_true', help=\"Train only the encoder\")\n",
    "    parser.add_argument('--gauss_std', default=(1), help=\"Gaussian noise std\")\n",
    "\n",
    "    return parser\n",
    "\n",
    "\n",
    "def build_dataset(dataset, batch_size, num_workers, test=False):\n",
    "    loader = get_data_loader(dataset,\n",
    "                             batch_size=batch_size,\n",
    "                             num_workers=num_workers,\n",
    "                             pin_mem=True,\n",
    "                             shuffle=not (test),\n",
    "                             drop_last=not (test))\n",
    "    return loader\n",
    "\n",
    "def build_model_enc_dec(model_str, device, args):\n",
    "\n",
    "    teacher = load_model(args.teacher_ckpt, device)\n",
    "    teacher.eval()\n",
    "    \n",
    "    model = deepcopy(teacher)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    model_kd = eval(model_str)\n",
    "    model_kd.to(device)\n",
    "    model_kd.eval()\n",
    "\n",
    "    if args.ckpt:\n",
    "        ckpt = torch.load(args.ckpt, map_location='cpu')\n",
    "        model_kd.load_state_dict(ckpt['model'], strict=True)\n",
    "        args.start_epoch = ckpt['epoch']\n",
    "        model_kd.train()\n",
    "\n",
    "    if args.encoder_only:\n",
    "        model.patch_embed = deepcopy(model_kd.patch_embed)\n",
    "        model.mask_generator = deepcopy(model_kd.mask_generator)\n",
    "        model.rope = deepcopy(model_kd.rope)\n",
    "        model.enc_blocks = deepcopy(model_kd.enc_blocks)\n",
    "        model.enc_norm = deepcopy(model_kd.enc_norm)\n",
    "        model.adapter = deepcopy(model_kd.adapter)\n",
    "    else:\n",
    "        model = model_kd\n",
    "\n",
    "    return teacher, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = get_args_parser()\n",
    "args = args.parse_args('')\n",
    "global device\n",
    "device = f\"cuda:{args.cuda}\" if args.cuda >= 0 else \"cpu\"\n",
    "device = torch.device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DATA =  \"100 @ Co3d(split='test', ROOT='/ssd1/sa58728/dust3r/data/co3d_subset_processed', resolution=224, seed=777, gauss_std=3)\"\n",
    "# TEST_DATA = \"100 @ ScanNet(split='test', ROOT='/ssd1/wenyan/scannetpp_processed', resolution=224, seed=777, gauss_std=3)\"\n",
    "# TEST_DATA = \"100 @ DL3DV(split='test', ROOT='/ssd1/sa58728/dust3r/data/DL3DV-10K', resolution=224, seed=777, gauss_std=3)\"\n",
    "data_loader = {dataset.split('(')[0]: build_dataset(dataset, args.batch_size, args.num_workers, test=True)\n",
    "                    for dataset in TEST_DATA.split('+')}\n",
    "\n",
    "model_dims = [384, 6, 768, 12]\n",
    "MODEL_KD = \"AsymmetricCroCo3DStereo(pos_embed='RoPE100', img_size=(224, 224), head_type='dpt', \\\n",
    "            output_mode='pts3d', depth_mode=('exp', -inf, inf), conf_mode=('exp', 1, inf), \\\n",
    "            enc_embed_dim={}, enc_depth=12, enc_num_heads={}, dec_embed_dim={}, dec_depth=12, dec_num_heads={}, adapter=True)\".format(*model_dims)\n",
    "teacher, model = build_model_enc_dec(MODEL_KD, device, args)\n",
    "\n",
    "TEST_CRITERION = f\"ConfLoss(Regr3D(L21, norm_mode='avg_dis', kd=True, roma=1, debug=True, device=device), alpha=0.2) + \\\n",
    "                   Regr3D_ScaleShiftInv(L21, gt_scale=True, kd=True, roma=1, debug=True, device=device)\"\n",
    "criterion = eval(TEST_CRITERION).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_name, testset = tuple(data_loader.items())[0]\n",
    "model.eval()\n",
    "metric_logger = misc.MetricLogger(delimiter=\"  \")\n",
    "metric_logger.meters = defaultdict(lambda: misc.SmoothedValue(window_size=9**9))\n",
    "\n",
    "if hasattr(testset, 'dataset') and hasattr(testset.dataset, 'set_epoch'):\n",
    "    testset.dataset.set_epoch(0)\n",
    "if hasattr(testset, 'sampler') and hasattr(testset.sampler, 'set_epoch'):\n",
    "    testset.sampler.set_epoch(0)\n",
    "\n",
    "hi = {'roma_mse': 0}\n",
    "lo = {'roma_mse': 1e9}\n",
    "for _, batch in tqdm(enumerate(testset), total=len(testset)):\n",
    "    with torch.no_grad():\n",
    "        loss_tuple = loss_of_one_batch(batch, model, criterion, device, symmetrize_batch=True, features=args.kd_enc, \n",
    "                                    ret='loss', kd_enc=args.kd_enc, kd_out=args.kd_out, teacher=teacher, lmd=args.lmd, roma=args.roma)\n",
    "    loss_value, loss_details = loss_tuple\n",
    "    # metric_logger.update(loss=float(loss_value), **loss_details)\n",
    "    if loss_details['roma_mse'] > hi['roma_mse']:\n",
    "        hi = loss_details\n",
    "        hi_batch = batch\n",
    "    if loss_details['roma_mse'] < lo['roma_mse']:\n",
    "        lo = loss_details\n",
    "        lo_batch = batch\n",
    "\n",
    "# gather the stats from all processes\n",
    "# metric_logger.synchronize_between_processes()\n",
    "# print(\"Averaged stats:\", metric_logger)\n",
    "# aggs = [('avg', 'global_avg'), ('med', 'median')]\n",
    "# results = {f'{k}_{tag}': getattr(meter, attr) for k, meter in metric_logger.meters.items() for tag, attr in aggs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi['roma_mse'], hi['roma_mae'], lo['roma_mse'], lo['roma_mae']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1, p2, cert = lo['roma_details']\n",
    "batch = lo_batch\n",
    "p1 = p1.reshape(-1,448,224,3)\n",
    "p2 = p2.reshape(-1,448,224,3)\n",
    "cert = cert.reshape(-1,448,224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "idx = 0\n",
    "\n",
    "im_concat = torch.cat((batch[0]['img'][idx], batch[1]['img'][idx]), dim=1)\n",
    "plt.imshow(im_concat.permute(1,2,0).cpu().numpy() * 0.5 + 0.5)\n",
    "\n",
    "plt.imshow(cert[idx].cpu().numpy(), alpha=0.3)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[0]['img_path'], batch[1]['img_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(p1[idx,:,:,2].cpu().numpy())\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "plt.imshow(p2[idx,:,:,2].cpu().numpy())\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "plt.imshow(np.abs(p1[idx,:,:,2].cpu().numpy() - p2[idx,:,:,2].cpu().numpy()))\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cert1 = cert[idx].cpu().numpy() > 0.7\n",
    "\n",
    "plt.imshow(p2[idx,:,:,2].cpu().numpy() * cert1)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(np.abs(p1[idx,:,:,2].cpu().numpy() - p2[idx,:,:,2].cpu().numpy()) * cert1)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((np.abs(p1[0,:,:,2].cpu().numpy() - p2[0,:,:,2].cpu().numpy()) * cert1).mean() / cert1.mean())\n",
    "print((((p1[0,:,:,2].cpu().numpy() - p2[0,:,:,2].cpu().numpy())**2) * cert1).mean() / cert1.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.abs(p1[idx,:,:,2].cpu().numpy() - p2[idx,:,:,2].cpu().numpy() * cert1).ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(lo['pred'][0]['conf'][0].detach().cpu().numpy() > 2) # ) / hi['pred'][0]['conf'][0].detach().cpu().numpy().max())\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cert[idx].cpu().numpy()[:224])\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(hi['pred'][0]['conf'][0].detach().cpu().numpy() / hi['pred'][0]['conf'][0].detach().cpu().numpy().max() * cert[idx].cpu().numpy()[:224])\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cert > 0.5 must have more than 1% of the pixels to have enough overlap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Any Video to Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, cv2\n",
    "from pytube import YouTube\n",
    "\n",
    "def extract_frames(input_path, start=0, end=-1, subsample=1):\n",
    "  output_file = os.path.basename(input_path).split('.')[0]\n",
    "  output_path = os.path.join(os.path.dirname(input_path), output_file)\n",
    "  os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "  vidcap = cv2.VideoCapture(input_path)\n",
    "  vidcap.set(cv2.CAP_PROP_POS_FRAMES, start)\n",
    "  success, image = vidcap.read()\n",
    "\n",
    "  count = start\n",
    "  while success:\n",
    "    if count % subsample == 0:\n",
    "      cv2.imwrite(f\"{output_path}/{count:>05}.jpg\", image) \n",
    "    success, image = vidcap.read()\n",
    "    count += 1\n",
    "    if count >= end:\n",
    "      break\n",
    "\n",
    "def us_to_frames(start, end, fps):\n",
    "  return int(start * fps / 1e6), int(end * fps / 1e6)\n",
    "\n",
    "def process_file(input_path, hd=False, thresh_s=0, subsample=1):\n",
    "  output_path = os.path.dirname(input_path)\n",
    "  output_file = os.path.basename(input_path).split('.')[0]\n",
    "\n",
    "  with open(input_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "  vid = lines[0].strip()\n",
    "  print(vid)\n",
    "  yt = YouTube(vid)\n",
    "  try:\n",
    "    streams = yt.streams.filter(progressive=True, file_extension='mp4').order_by('resolution')\n",
    "  except:\n",
    "    print(f\"Video not found\")\n",
    "    return 1\n",
    "  if hd:\n",
    "    stream = streams.last()\n",
    "    fps = yt.vid_info['streamingData']['formats'][-1]['fps'] # 1/s\n",
    "  else:\n",
    "    stream = streams.first()\n",
    "    fps = yt.vid_info['streamingData']['formats'][0]['fps'] # 1/s\n",
    "    \n",
    "  start = int(lines[1].split(' ')[0])\n",
    "  end = int(lines[-1].split(' ')[0])\n",
    "  print(f\"Start: {start}, End: {end}, {end-start} us\")\n",
    "  start, end = us_to_frames(start, end, fps)\n",
    "  print(f\"Start: {start}, End: {end}, {end-start} frames -> {(end-start)//subsample} frames\")\n",
    "  if (end-start) / fps < thresh_s:\n",
    "    print(f\"Video too short\")\n",
    "    return 1\n",
    "\n",
    "  stream.download(output_path=output_path, filename=f'{output_file}.mp4')\n",
    "\n",
    "  extract_frames(f\"{output_path}/{output_file}.mp4\", start, end, subsample)\n",
    "  os.remove(f\"{output_path}/{output_file}.mp4\")\n",
    "  return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/ssd1/sa58728/dust3r/data/acid/train/'\n",
    "for file in os.listdir(data_path):\n",
    "  if file.endswith('.txt'):\n",
    "    print(file)\n",
    "    flag = process_file(os.path.join(data_path, file), hd=True, thresh_s=0, subsample=3)\n",
    "    print()\n",
    "    # if not flag:\n",
    "    #   break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DTU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dust3r.datasets import get_data_loader  # noqa\n",
    "\n",
    "def build_dataset(dataset, batch_size, num_workers, test=False):\n",
    "    loader = get_data_loader(dataset,\n",
    "                             batch_size=batch_size,\n",
    "                             num_workers=num_workers,\n",
    "                             pin_mem=False,\n",
    "                             shuffle=not (test),\n",
    "                             drop_last=not (test))\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DATA = \"DTU(split='train', ROOT='/ssd1/sa58728/dust3r/data/dtu_processed', resolution=224, seed=777, gauss_std=3)\"\n",
    "TEST_DATA = \"Co3d(split='test', ROOT='/ssd1/sa58728/dust3r/data/co3d_subset_processed', resolution=224, seed=777, gauss_std=3)\"\n",
    "\n",
    "dataset = build_dataset(TEST_DATA, 1, 1, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataset:\n",
    "  print(batch[1]['img'].shape)\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(batch[1]['valid_mask'][0].cpu().numpy())\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(batch[1]['img'][0].permute(1,2,0).cpu().numpy() * 0.5 + 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(batch[1]['valid_mask'][0].cpu().numpy())\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(batch[1]['pts3d'][0,...,2].cpu().numpy() * batch[1]['valid_mask'][0].cpu().numpy())\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "batch[1]['pts3d'][0,...,2].cpu().numpy().min(), batch[1]['pts3d'][0,...,2].cpu().numpy().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[1]['pts3d'][0,...,2].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[1].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[1]['camera_intrinsics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[1]['img_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = np.array(Image.open('/ssd1/sa58728/dust3r/data/Depths/scan3/depth_visual_0014.png'))\n",
    "plt.imshow(depth)\n",
    "depth.min(), depth.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = np.array(Image.open('/ssd1/sa58728/dust3r/data/dtu_processed/Depths/scan3_0/depth_map_0.png'))\n",
    "plt.imshow(depth)\n",
    "depth.min(), depth.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.preprocess_dtu import read_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth, mask, depth_h, mask_h = read_depth('/ssd1/sa58728/dust3r/data/Depths/scan3/depth_map_0014.pfm')\n",
    "depth.shape, mask.shape, depth_h.shape, mask_h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(depth_h, cmap='inferno')\n",
    "depth_h.min(), depth_h.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_h /= 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_depth_map = (depth_h / np.max(depth_h) * 65535).astype(np.uint16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(scaled_depth_map, cmap='inferno')\n",
    "scaled_depth_map.min(), scaled_depth_map.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "cv2.imwrite('test.png', scaled_depth_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = np.array(Image.open('test.png'))\n",
    "plt.imshow(depth)\n",
    "depth.min(), depth.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test DTU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import Sized\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "torch.backends.cuda.matmul.allow_tf32 = True  # for gpu >= Ampere and pytorch >= 1.12\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dust3r.model import AsymmetricCroCo3DStereo, inf  # noqa: F401, needed when loading the model\n",
    "from dust3r.datasets import get_data_loader  # noqa\n",
    "from dust3r.losses import *  # noqa: F401, needed when loading the model\n",
    "from dust3r.inference import loss_of_one_batch, load_model\n",
    "import dust3r.utils.path_to_croco  # noqa: F401\n",
    "import croco.utils.misc as misc  # noqa\n",
    "\n",
    "sys.path.insert(0, './')\n",
    "from test_kd import get_args_parser, build_dataset, build_model_enc_dec \n",
    "\n",
    "H, W = 224, 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_plot(tensor, norm=None):\n",
    "    STD = np.array([0.229, 0.224, 0.225])\n",
    "    MEAN = np.array([0.485, 0.456, 0.406])\n",
    "    if len(tensor.shape) == 4:\n",
    "        tensor = tensor[0]\n",
    "    if len(tensor.shape) == 2:\n",
    "        return tensor.cpu().numpy()\n",
    "    elif norm == 'roma':\n",
    "        return tensor.cpu().permute(1,2,0).numpy() * STD + MEAN\n",
    "    elif norm == 'dust3r':\n",
    "        return (tensor.cpu().permute(1,2,0).numpy() * 0.5) + 0.5\n",
    "    return tensor.cpu().permute(1,2,0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = get_args_parser()\n",
    "args = args.parse_args('')\n",
    "args.cuda = 2\n",
    "# SETUP\n",
    "misc.init_distributed_mode(args)\n",
    "device = f\"cuda:{args.cuda}\" if args.cuda >= 0 else \"cpu\"\n",
    "device = torch.device(device)\n",
    "seed = args.seed\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "cudnn.benchmark = False\n",
    "cudnn.deterministic = True\n",
    "\n",
    "# DATA\n",
    "# TEST_DATA =  f\"Co3d(split='test', ROOT='/ssd1/sa58728/dust3r/data/co3d_subset_processed', resolution=224, seed=777, gauss_std={args.gauss_std})\"\n",
    "# TEST_DATA += f\"+ ScanNet(split='test', ROOT='/ssd1/wenyan/scannetpp_processed', resolution=224, seed=777, gauss_std={args.gauss_std})\"\n",
    "# TEST_DATA += f\"+ DL3DV(split='test', ROOT='/ssd1/sa58728/dust3r/data/DL3DV-10K', resolution=224, seed=777, gauss_std={args.gauss_std})\"\n",
    "TEST_DATA = f\"BlendedMVS(split='train', ROOT='/ssd1/sa58728/dust3r/data/blendedmvs_processed', resolution=224, seed=777, gauss_std={args.gauss_std})\"\n",
    "\n",
    "data_loader_test = {dataset.split('(')[0]: build_dataset(dataset, args.batch_size, args.num_workers, test=True)\n",
    "                    for dataset in TEST_DATA.split('+')}\n",
    "\n",
    "# MODEL\n",
    "if args.encoder_only:\n",
    "    model_dims = [384, 6, 768, 12]\n",
    "else:\n",
    "    model_dims = [384, 6, 192, 3]\n",
    "MODEL_KD = \"AsymmetricCroCo3DStereo(pos_embed='RoPE100', img_size=(224, 224), head_type='dpt', \\\n",
    "            output_mode='pts3d', depth_mode=('exp', -inf, inf), conf_mode=('exp', 1, inf), \\\n",
    "            enc_embed_dim={}, enc_depth=12, enc_num_heads={}, dec_embed_dim={}, dec_depth=12, dec_num_heads={}, adapter=True)\".format(*model_dims)\n",
    "teacher, model = build_model_enc_dec(MODEL_KD, device, args)\n",
    "if args.distributed:\n",
    "    model = torch.nn.parallel.DistributedDataParallel(\n",
    "        model, device_ids=[args.gpu], find_unused_parameters=True, static_graph=True)\n",
    "    \n",
    "# CRITERION\n",
    "TEST_CRITERION = f\"ConfLoss(Regr3D(L21, norm_mode='avg_dis', kd={args.kd_out}, asimmetric={args.asimmetric}), alpha=0.2) + \\\n",
    "                   Regr3D_ScaleShiftInv(L21, gt_scale=True, kd={args.kd_out}, roma={args.roma}, \\\n",
    "                   asimmetric=False, debug=True, device=device)\"\n",
    "test_criterion = eval(TEST_CRITERION).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_name, testset in data_loader_test.items():\n",
    "    print(test_name)\n",
    "    model.eval()\n",
    "    metric_logger = misc.MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.meters = defaultdict(lambda: misc.SmoothedValue(window_size=9**9))\n",
    "    header = 'Test Epoch: [{}]\\n>'.format(0)\n",
    "\n",
    "    if hasattr(testset, 'dataset') and hasattr(testset.dataset, 'set_epoch'):\n",
    "        testset.dataset.set_epoch(0)\n",
    "    if hasattr(testset, 'sampler') and hasattr(testset.sampler, 'set_epoch'):\n",
    "        testset.sampler.set_epoch(0)\n",
    "\n",
    "    for _, batch in enumerate(testset):\n",
    "        view1, view2 = batch\n",
    "        for view in batch:\n",
    "            for name in 'img pts3d valid_mask camera_pose camera_intrinsics F_matrix corres'.split():  # pseudo_focal\n",
    "                if name not in view:\n",
    "                    continue\n",
    "                view[name] = view[name].to(device, non_blocking=True)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=True):\n",
    "            outs = model(view1, view2, features=True)\n",
    "            pred1, pred2 = outs\n",
    "\n",
    "            # loss is supposed to be symmetric\n",
    "            with torch.cuda.amp.autocast(enabled=False):\n",
    "                loss = test_criterion(view1, view2, pred1, pred2) if test_criterion is not None else None\n",
    "                loss_tot = loss[0].clone()\n",
    "                loss_dict = loss[1].copy()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            teacher_outs = teacher(view1, view2, features=True)\n",
    "\n",
    "        loss_kd = 0\n",
    "        for pred_side, teacher_side in zip(outs, teacher_outs): # for each view\n",
    "            pred_feats = pred_side['features']\n",
    "            target_feats = teacher_side['features']\n",
    "            loss_kd += torch.nn.MSELoss()(pred_feats, target_feats) / 2\n",
    "            loss_tot += loss_kd * args.lmd\n",
    "        \n",
    "        loss_dict['kd'] = loss_kd.item()\n",
    "\n",
    "        loss = (loss_tot, loss_dict)\n",
    "\n",
    "        result = dict(view1=view1, view2=view2, pred1=pred1, pred2=pred2, loss=loss)\n",
    "        result['teacher_outs'] = teacher_outs\n",
    "\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['loss'][1].keys()\n",
    "g1, g2, p1, p2, m1, m2 = result['loss'][1]['scaled_outs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['view1'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(tensor_to_plot(result['view2']['img'][0], norm='dust3r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(tensor_to_plot(result['view2']['pts3d'][0,...,2]))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['view2']['camera_pose'][0], result['view2']['camera_intrinsics'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['view1']['camera_pose'][0], result['view1']['camera_intrinsics'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(tensor_to_plot(result['pred1']['pts3d'][0,...,-1].detach()))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(tensor_to_plot(result['pred2']['pts3d_in_other_view'][0,...,-1].detach()))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(tensor_to_plot(m2[0].detach()))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(tensor_to_plot(g1[0,...,-1].detach() * m1[0].detach()))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(tensor_to_plot(g2[0,...,-1].detach()))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(tensor_to_plot(p1[0,...,-1].detach()))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(tensor_to_plot(p2[0,...,-1].detach()))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(tensor_to_plot(p1[0,...,-1].detach() - g1[0,...,-1].detach()))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(tensor_to_plot(view1['pts3d'][0,...,-1].detach()))\n",
    "plt.colorbar()\n",
    "view1['pts3d'][0,...,0].min(), view1['pts3d'][0,...,0].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BlendedMVS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "VIEWS = 2\n",
    "\n",
    "def gen_blendedmvs_path(blendedmvs_data_folder, mode='training_mvs'):\n",
    "    \"\"\" generate data paths for blendedmvs dataset \"\"\"\n",
    "\n",
    "    # read data list\n",
    "    if mode == 'train':\n",
    "        proj_list = open(os.path.join(blendedmvs_data_folder, 'BlendedMVS_training.txt')).read().splitlines()\n",
    "    elif mode == 'val':\n",
    "        proj_list = open(os.path.join(blendedmvs_data_folder, 'validation_list.txt')).read().splitlines()\n",
    "\n",
    "    # parse all data\n",
    "    mvs_input_list = []\n",
    "    for data_name in proj_list:\n",
    "\n",
    "        dataset_folder = os.path.join(blendedmvs_data_folder, data_name)\n",
    "\n",
    "        # read cluster\n",
    "        cluster_path = os.path.join(dataset_folder, 'cams', 'pair.txt')\n",
    "        cluster_lines = open(cluster_path).read().splitlines()\n",
    "        image_num = int(cluster_lines[0])\n",
    "\n",
    "        # get per-image info\n",
    "        for idx in range(0, image_num):\n",
    "\n",
    "            ref_idx = int(cluster_lines[2 * idx + 1])\n",
    "            cluster_info =  cluster_lines[2 * idx + 2].split()\n",
    "            total_view_num = int(cluster_info[0])\n",
    "            if total_view_num < VIEWS - 1:\n",
    "                continue\n",
    "            paths = []\n",
    "            ref_image_path = os.path.join(dataset_folder, 'blended_images', '%08d_masked.jpg' % ref_idx)\n",
    "            ref_depth_path = os.path.join(dataset_folder, 'rendered_depth_maps', '%08d.pfm' % ref_idx)\n",
    "            ref_cam_path = os.path.join(dataset_folder, 'cams', '%08d_cam.txt' % ref_idx)\n",
    "            paths.append(ref_image_path)\n",
    "            paths.append(ref_cam_path)\n",
    "            paths.append(ref_depth_path)\n",
    "\n",
    "            for cidx in range(0, VIEWS - 1):\n",
    "                view_idx = int(cluster_info[2 * cidx + 1])\n",
    "                view_image_path = os.path.join(dataset_folder, 'blended_images', '%08d_masked.jpg' % view_idx)\n",
    "                view_depth_path = os.path.join(dataset_folder, 'rendered_depth_maps', '%08d.pfm' % view_idx)\n",
    "                view_cam_path = os.path.join(dataset_folder, 'cams', '%08d_cam.txt' % view_idx)\n",
    "                paths.append(view_image_path)\n",
    "                paths.append(view_cam_path)\n",
    "                paths.append(view_depth_path)\n",
    "\n",
    "            mvs_input_list.append(paths)\n",
    "\n",
    "    return mvs_input_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "MAX_D = 128\n",
    "\n",
    "def load_cam(file, interval_scale=1):\n",
    "    \"\"\" read camera txt file \"\"\"\n",
    "    words = file.read().split()\n",
    "\n",
    "    # read extrinsic\n",
    "    extr = np.zeros((4, 4))\n",
    "    for i in range(0, 4):\n",
    "        for j in range(0, 4):\n",
    "            extrinsic_index = 4 * i + j + 1\n",
    "            extr[i][j] = words[extrinsic_index]\n",
    "\n",
    "    # read intrinsic\n",
    "    intr = np.zeros((3, 3))\n",
    "    for i in range(0, 3):\n",
    "        for j in range(0, 3):\n",
    "            intrinsic_index = 3 * i + j + 18\n",
    "            intr[i][j] = words[intrinsic_index]\n",
    "\n",
    "    info = np.zeros((4))\n",
    "    if len(words) == 29:\n",
    "        info[0] = words[27]\n",
    "        info[1] = float(words[28]) * interval_scale\n",
    "        info[2] = MAX_D\n",
    "        info[3] = info[0] + info[1] * info[2]\n",
    "    elif len(words) == 30:\n",
    "        info[0] = words[27]\n",
    "        info[1] = float(words[28]) * interval_scale\n",
    "        info[2] = words[29]\n",
    "        info[3] = info[0] + info[1] * info[2]\n",
    "    elif len(words) == 31:\n",
    "        info[0] = words[27]\n",
    "        info[1] = float(words[28]) * interval_scale\n",
    "        info[2] = words[29]\n",
    "        info[3] = words[30]\n",
    "\n",
    "    return extr, intr, info\n",
    "\n",
    "def load_pfm(file):\n",
    "    color = None\n",
    "    width = None\n",
    "    height = None\n",
    "    scale = None\n",
    "    data_type = None\n",
    "    header = file.readline().decode('UTF-8').rstrip()\n",
    "\n",
    "    if header == 'PF':\n",
    "        color = True\n",
    "    elif header == 'Pf':\n",
    "        color = False\n",
    "    else:\n",
    "        raise Exception('Not a PFM file.')\n",
    "    dim_match = re.match(r'^(\\d+)\\s(\\d+)\\s$', file.readline().decode('UTF-8'))\n",
    "    if dim_match:\n",
    "        width, height = map(int, dim_match.groups())\n",
    "    else:\n",
    "        raise Exception('Malformed PFM header.')\n",
    "    # scale = float(file.readline().rstrip())\n",
    "    scale = float((file.readline()).decode('UTF-8').rstrip())\n",
    "    if scale < 0: # little-endian\n",
    "        data_type = '<f'\n",
    "    else:\n",
    "        data_type = '>f' # big-endian\n",
    "    data_string = file.read()\n",
    "    data = np.fromstring(data_string, data_type)\n",
    "    shape = (height, width, 3) if color else (height, width)\n",
    "    data = np.reshape(data, shape)\n",
    "    data = cv2.flip(data, 0)\n",
    "    return data\n",
    "\n",
    "\n",
    "def _read_pfm_disp(filename):\n",
    "    disp = np.ascontiguousarray(_read_pfm(filename)[0])\n",
    "    disp[disp<=0] = np.inf # eg /nfs/data/ffs-3d/datasets/middlebury/2014/Shopvac-imperfect/disp0.pfm\n",
    "    return disp\n",
    "\n",
    "def _read_pfm(file):\n",
    "    file = open(file, 'rb')\n",
    "\n",
    "    color = None\n",
    "    width = None\n",
    "    height = None\n",
    "    scale = None\n",
    "    endian = None\n",
    "\n",
    "    header = file.readline().rstrip()\n",
    "    if header.decode(\"ascii\") == 'PF':\n",
    "        color = True\n",
    "    elif header.decode(\"ascii\") == 'Pf':\n",
    "        color = False\n",
    "    else:\n",
    "        raise Exception('Not a PFM file.')\n",
    "\n",
    "    dim_match = re.match(r'^(\\d+)\\s(\\d+)\\s$', file.readline().decode(\"ascii\"))\n",
    "    if dim_match:\n",
    "        width, height = list(map(int, dim_match.groups()))\n",
    "    else:\n",
    "        raise Exception('Malformed PFM header.')\n",
    "\n",
    "    scale = float(file.readline().decode(\"ascii\").rstrip())\n",
    "    if scale < 0:  # little-endian\n",
    "        endian = '<'\n",
    "        scale = -scale\n",
    "    else:\n",
    "        endian = '>'  # big-endian\n",
    "\n",
    "    data = np.fromfile(file, endian + 'f')\n",
    "    shape = (height, width, 3) if color else (height, width)\n",
    "\n",
    "    data = np.reshape(data, shape)\n",
    "    data = np.flipud(data)\n",
    "    return data, scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "class MVSGenerator:\n",
    "    \"\"\" data generator class, tf only accept generator without param \"\"\"\n",
    "    def __init__(self, sample_list):\n",
    "        self.sample_list = sample_list\n",
    "        self.counter = 0\n",
    "    \n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            for data in self.sample_list: \n",
    "\n",
    "                ###### read input data ######\n",
    "                cams, images = [], []\n",
    "                for view in range(VIEWS):\n",
    "                    image = cv2.imread(data[2 * view])\n",
    "                    cam = load_cam(open(data[2 * view + 1]))\n",
    "                    images.append(image)\n",
    "                    cams.append(cam)\n",
    "                depth_image = _read_pfm_disp(data[2 * VIEWS])\n",
    "            yield (images, cams, depth_image) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lists = gen_blendedmvs_path('/ssd1/sa58728/dust3r/data/BlendedMVS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lists[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = MVSGenerator(lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (images, cams, depth_image) in enumerate(data):\n",
    "    if i == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images[0].shape, cams[1], depth_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(images[0])\n",
    "\n",
    "plt.imshow(depth_image, alpha=0.6)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "\n",
    "class CameraPoseVisualizer:\n",
    "    def __init__(self, xlim, ylim, zlim):\n",
    "        self.fig = plt.figure(figsize=(18, 7))\n",
    "        self.ax = self.fig.add_subplot(projection='3d')\n",
    "        self.ax.set_aspect(\"auto\")\n",
    "        self.ax.set_xlim(xlim)\n",
    "        self.ax.set_ylim(ylim)\n",
    "        self.ax.set_zlim(zlim)\n",
    "        self.ax.set_xlabel('x')\n",
    "        self.ax.set_ylabel('y')\n",
    "        self.ax.set_zlabel('z')\n",
    "        print('initialize camera pose visualizer')\n",
    "\n",
    "    def extrinsic2pyramid(self, extrinsic, color='r', focal_len_scaled=5, aspect_ratio=0.3):\n",
    "        vertex_std = np.array([[0, 0, 0, 1],\n",
    "                               [focal_len_scaled * aspect_ratio, -focal_len_scaled * aspect_ratio, focal_len_scaled, 1],\n",
    "                               [focal_len_scaled * aspect_ratio, focal_len_scaled * aspect_ratio, focal_len_scaled, 1],\n",
    "                               [-focal_len_scaled * aspect_ratio, focal_len_scaled * aspect_ratio, focal_len_scaled, 1],\n",
    "                               [-focal_len_scaled * aspect_ratio, -focal_len_scaled * aspect_ratio, focal_len_scaled, 1]])\n",
    "        vertex_transformed = vertex_std @ extrinsic.T\n",
    "        meshes = [[vertex_transformed[0, :-1], vertex_transformed[1][:-1], vertex_transformed[2, :-1]],\n",
    "                            [vertex_transformed[0, :-1], vertex_transformed[2, :-1], vertex_transformed[3, :-1]],\n",
    "                            [vertex_transformed[0, :-1], vertex_transformed[3, :-1], vertex_transformed[4, :-1]],\n",
    "                            [vertex_transformed[0, :-1], vertex_transformed[4, :-1], vertex_transformed[1, :-1]],\n",
    "                            [vertex_transformed[1, :-1], vertex_transformed[2, :-1], vertex_transformed[3, :-1], vertex_transformed[4, :-1]]]\n",
    "        self.ax.add_collection3d(\n",
    "            Poly3DCollection(meshes, facecolors=color, linewidths=0.3, edgecolors=color, alpha=0.35))\n",
    "\n",
    "    def customize_legend(self, list_label):\n",
    "        list_handle = []\n",
    "        for idx, label in enumerate(list_label):\n",
    "            color = plt.cm.rainbow(idx / len(list_label))\n",
    "            patch = Patch(color=color, label=label)\n",
    "            list_handle.append(patch)\n",
    "        plt.legend(loc='right', bbox_to_anchor=(1.8, 0.5), handles=list_handle)\n",
    "\n",
    "    def colorbar(self, max_frame_length):\n",
    "        cmap = mpl.cm.rainbow\n",
    "        norm = mpl.colors.Normalize(vmin=0, vmax=max_frame_length)\n",
    "        self.fig.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=cmap), orientation='vertical', label='Frame Number')\n",
    "\n",
    "    def show(self):\n",
    "        plt.title('Extrinsic Parameters')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extr = np.array(\n",
    "    [[ 0.0693598 , -0.00333963,  0.997586  , -0.318076  ],\n",
    "     [ 0.550796  ,  0.833885  , -0.0355039 , -0.324893  ],\n",
    "     [-0.831753  ,  0.551929  ,  0.0596776 ,  0.475446  ],\n",
    "     [ 0.        ,  0.        ,  0.        ,  1.        ]])\n",
    "\n",
    "visualizer = CameraPoseVisualizer([-5, 5], [-5, 5], [0, 5])\n",
    "visualizer.extrinsic2pyramid(extr, 'c', 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import random\n",
    "import numpy as np\n",
    "from test_kd import get_args_parser, build_dataset, build_model_enc_dec \n",
    "\n",
    "args = get_args_parser()\n",
    "args = args.parse_args('')\n",
    "\n",
    "H, W = 224, 224\n",
    "TEST_DATA = f\"BlendedMVS(split='train', ROOT='/ssd1/sa58728/dust3r/data/blendedmvs_processed', resolution=224, seed=777, gauss_std={args.gauss_std})\"\n",
    "\n",
    "args = get_args_parser()\n",
    "args = args.parse_args('')\n",
    "\n",
    "# SETUP\n",
    "seed = args.seed\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "def tensor_to_plot(tensor, norm=None):\n",
    "    STD = np.array([0.229, 0.224, 0.225])\n",
    "    MEAN = np.array([0.485, 0.456, 0.406])\n",
    "    if len(tensor.shape) == 4:\n",
    "        tensor = tensor[0]\n",
    "    if len(tensor.shape) == 2:\n",
    "        return tensor.cpu().numpy()\n",
    "    elif norm == 'roma':\n",
    "        return tensor.cpu().permute(1,2,0).numpy() * STD + MEAN\n",
    "    elif norm == 'dust3r':\n",
    "        return (tensor.cpu().permute(1,2,0).numpy() * 0.5) + 0.5\n",
    "    return tensor.cpu().permute(1,2,0).numpy()\n",
    "\n",
    "# DATA\n",
    "# TEST_DATA =  f\"Co3d(split='test', ROOT='/ssd1/sa58728/dust3r/data/co3d_subset_processed', resolution=224, seed=777, gauss_std={args.gauss_std})\"\n",
    "# TEST_DATA += f\"+ ScanNet(split='test', ROOT='/ssd1/wenyan/scannetpp_processed', resolution=224, seed=777, gauss_std={args.gauss_std})\"\n",
    "# TEST_DATA += f\"+ DL3DV(split='test', ROOT='/ssd1/sa58728/dust3r/data/DL3DV-10K', resolution=224, seed=777, gauss_std={args.gauss_std})\"\n",
    "TEST_DATA = f\"BlendedMVS(split='train', ROOT='/ssd1/sa58728/dust3r/data/blendedmvs_processed', resolution=224, seed=777, gauss_std={args.gauss_std})\"\n",
    "\n",
    "data_loader_test = {dataset.split('(')[0]: build_dataset(dataset, args.batch_size, args.num_workers, test=True)\n",
    "                    for dataset in TEST_DATA.split('+')}\n",
    "\n",
    "for test_name, testset in data_loader_test.items():\n",
    "    print(test_name)\n",
    "    if hasattr(testset, 'dataset') and hasattr(testset.dataset, 'set_epoch'):\n",
    "        testset.dataset.set_epoch(0)\n",
    "    if hasattr(testset, 'sampler') and hasattr(testset.sampler, 'set_epoch'):\n",
    "        testset.sampler.set_epoch(0)\n",
    "\n",
    "    for _, batch in enumerate(testset):\n",
    "        view1, view2 = batch\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = tensor_to_plot(view1['img'][0], norm='dust3r')\n",
    "pts = tensor_to_plot(view1['pts3d'][0,...,-1].detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(tensor_to_plot(view1['pts3d'][0,...,1].detach()))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(tensor_to_plot(view1['depthmap'][0]))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2,5,7,10,14])\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "a, np.log(a), sigmoid(np.log(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dust3r",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
